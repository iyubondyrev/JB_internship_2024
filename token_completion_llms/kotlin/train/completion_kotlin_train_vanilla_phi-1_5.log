Model has a total of 124645632 trainable parameters
Training/evaluation parameters Namespace(data_dir='../../../datasets/kotlin', langs='kotlin', output_dir='save/', model_type='gpt2', pretrain_dir='microsoft/CodeGPT-small-py-adaptedGPT2', config_dir=None, tokenizer_dir=None, lit_file='../../../datasets/kotlin/literals.json', load_name='pretrained', mlm=False, mlm_probability=0.15, cache_dir='', block_size=1024, do_train=True, do_eval=False, evaluate_during_training=True, do_lower_case=False, per_gpu_train_batch_size=2, per_gpu_eval_batch_size=4, gradient_accumulation_steps=4, learning_rate=8e-05, weight_decay=0.01, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=5.0, max_steps=-1, warmup_steps=0, logging_steps=100, save_steps=1000, save_total_limit=None, eval_all_checkpoints=False, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=42, not_pretrain=True, fp16=False, fp16_opt_level='O1', local_rank=-1, node_index=-1, gpu_per_node=1, server_ip='', server_port='', log_file='completion_kotlin_eval.log', tensorboard_dir=None, wandb_run_name='EXP_1', wandb_project_name='JB_internship_2024', n_gpu=1, device=device(type='cuda'), start_epoch=0, start_step=0)
Creating features from dataset file at ../../../datasets/kotlin/train.txt
Data size: 39754
Rank 0, load 0
Rank 0, load 10
Rank 0, load 20
Rank 0, load 30
Rank 0, load 40
Rank 0, load 50
Rank 0, load 60
Rank 0, load 70
Rank 0, load 80
Rank 0, load 90
Rank 0, load 100
tokens: 22796815
Rank 0 Training 22796815 token, 22262 samples
Saving features into cached file save/train_blocksize_1024_wordsize_1_rank_0
***** Running training *****
  Num examples = 22262
  Num epoch = 4
  Instantaneous batch size per GPU = 2
  Total train batch size (w. parallel, distributed & accumulation) = 8
  Gradient Accumulation steps = 4
  Total optimization steps = 13910
Data size: 2487
load 0
load 10
load 20
load 30
load 40
load 50
load 60
load 70
load 80
load 90
load 100
tokens: 1496828
0 are done!
2233, 0.451858486341245
100 are done!
261649, 0.36323853712416254
200 are done!
527468, 0.36723175623924104
300 are done!
804476, 0.41510747368473394
Eval on 2487, saved at save/predictions.txt
Total 867460 tokens, accuracy: 45.42
  steps: 100  ppl: 5.5377  lr: 7.942487419122934e-05
  steps: 200  ppl: 3.7474  lr: 7.884974838245867e-05
Model has a total of 124645632 trainable parameters
Training/evaluation parameters Namespace(data_dir='../../../datasets/kotlin', langs='kotlin', output_dir='save/', model_type='gpt2', pretrain_dir='microsoft/CodeGPT-small-py-adaptedGPT2', config_dir=None, tokenizer_dir=None, lit_file='../../../datasets/kotlin/literals.json', load_name='pretrained', mlm=False, mlm_probability=0.15, cache_dir='', block_size=1024, do_train=True, do_eval=False, evaluate_during_training=True, do_lower_case=False, per_gpu_train_batch_size=16, per_gpu_eval_batch_size=16, gradient_accumulation_steps=2, learning_rate=8e-05, weight_decay=0.01, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=5.0, max_steps=-1, warmup_steps=0, logging_steps=100, save_steps=1000, save_total_limit=None, eval_all_checkpoints=False, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=42, not_pretrain=True, fp16=False, fp16_opt_level='O1', local_rank=-1, node_index=-1, gpu_per_node=1, server_ip='', server_port='', log_file='completion_kotlin_eval.log', tensorboard_dir=None, wandb_run_name='EXP_1', wandb_project_name='JB_internship_2024', n_gpu=1, device=device(type='cuda'), start_epoch=0, start_step=0)
Loading features from cached file save/train_blocksize_1024_wordsize_1_rank_0
***** Running training *****
  Num examples = 22262
  Num epoch = 4
  Instantaneous batch size per GPU = 16
  Total train batch size (w. parallel, distributed & accumulation) = 32
  Gradient Accumulation steps = 2
  Total optimization steps = 3475
0 are done!
9830, 0.35615462868769077
Eval on 2487, saved at save/predictions.txt
Total 867460 tokens, accuracy: 45.42
Model has a total of 124645632 trainable parameters
Training/evaluation parameters Namespace(data_dir='../../../datasets/kotlin', langs='kotlin', output_dir='save/', model_type='gpt2', pretrain_dir='microsoft/CodeGPT-small-py-adaptedGPT2', config_dir=None, tokenizer_dir=None, lit_file='../../../datasets/kotlin/literals.json', load_name='pretrained', mlm=False, mlm_probability=0.15, cache_dir='', block_size=1024, do_train=True, do_eval=False, evaluate_during_training=True, do_lower_case=False, per_gpu_train_batch_size=12, per_gpu_eval_batch_size=12, gradient_accumulation_steps=3, learning_rate=8e-05, weight_decay=0.01, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=5.0, max_steps=-1, warmup_steps=0, logging_steps=100, save_steps=1000, save_total_limit=None, eval_all_checkpoints=False, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=42, not_pretrain=True, fp16=False, fp16_opt_level='O1', local_rank=-1, node_index=-1, gpu_per_node=1, server_ip='', server_port='', log_file='completion_kotlin_eval.log', tensorboard_dir=None, wandb_run_name='EXP_1', wandb_project_name='JB_internship_2024', n_gpu=1, device=device(type='cuda'), start_epoch=0, start_step=0)
Loading features from cached file save/train_blocksize_1024_wordsize_1_rank_0
***** Running training *****
  Num examples = 22262
  Num epoch = 4
  Instantaneous batch size per GPU = 12
  Total train batch size (w. parallel, distributed & accumulation) = 36
  Gradient Accumulation steps = 3
  Total optimization steps = 3090
0 are done!
7357, 0.35829821938290063
100 are done!
809550, 0.41479216848866657
Eval on 2487, saved at save/predictions.txt
Total 867460 tokens, accuracy: 45.42
Model has a total of 124645632 trainable parameters
Training/evaluation parameters Namespace(data_dir='../../../datasets/kotlin', langs='kotlin', output_dir='save/', model_type='gpt2', pretrain_dir='microsoft/CodeGPT-small-py-adaptedGPT2', config_dir=None, tokenizer_dir=None, lit_file='../../../datasets/kotlin/literals.json', load_name='pretrained', mlm=False, mlm_probability=0.15, cache_dir='', block_size=1024, do_train=True, do_eval=False, evaluate_during_training=True, do_lower_case=False, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=8, gradient_accumulation_steps=4, learning_rate=8e-05, weight_decay=0.01, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=5.0, max_steps=-1, warmup_steps=0, logging_steps=100, save_steps=1000, save_total_limit=None, eval_all_checkpoints=False, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=42, not_pretrain=True, fp16=False, fp16_opt_level='O1', local_rank=-1, node_index=-1, gpu_per_node=1, server_ip='', server_port='', log_file='completion_kotlin_eval.log', tensorboard_dir=None, wandb_run_name='EXP_1', wandb_project_name='JB_internship_2024', n_gpu=1, device=device(type='cuda'), start_epoch=0, start_step=0)
Loading features from cached file save/train_blocksize_1024_wordsize_1_rank_0
***** Running training *****
  Num examples = 22262
  Num epoch = 4
  Instantaneous batch size per GPU = 8
  Total train batch size (w. parallel, distributed & accumulation) = 32
  Gradient Accumulation steps = 4
  Total optimization steps = 3475
0 are done!
5026, 0.38658973338639074
100 are done!
529928, 0.367398967406893
Eval on 2487, saved at save/predictions.txt
Total 867460 tokens, accuracy: 45.42
Model has a total of 124645632 trainable parameters
Training/evaluation parameters Namespace(data_dir='../../../datasets/kotlin', langs='kotlin', output_dir='save/', model_type='gpt2', pretrain_dir='microsoft/CodeGPT-small-py-adaptedGPT2', config_dir=None, tokenizer_dir=None, lit_file='../../../datasets/kotlin/literals.json', load_name='pretrained', mlm=False, mlm_probability=0.15, cache_dir='', block_size=1024, do_train=True, do_eval=False, evaluate_during_training=True, do_lower_case=False, per_gpu_train_batch_size=4, per_gpu_eval_batch_size=8, gradient_accumulation_steps=4, learning_rate=8e-05, weight_decay=0.01, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=5.0, max_steps=-1, warmup_steps=0, logging_steps=100, save_steps=1000, save_total_limit=None, eval_all_checkpoints=False, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=42, not_pretrain=True, fp16=False, fp16_opt_level='O1', local_rank=-1, node_index=-1, gpu_per_node=1, server_ip='', server_port='', log_file='completion_kotlin_eval.log', tensorboard_dir=None, wandb_run_name='EXP_1', wandb_project_name='JB_internship_2024', n_gpu=1, device=device(type='cuda'), start_epoch=0, start_step=0)
Loading features from cached file save/train_blocksize_1024_wordsize_1_rank_0
***** Running training *****
  Num examples = 22262
  Num epoch = 4
  Instantaneous batch size per GPU = 4
  Total train batch size (w. parallel, distributed & accumulation) = 16
  Gradient Accumulation steps = 4
  Total optimization steps = 6955
0 are done!
5026, 0.38658973338639074
100 are done!
529928, 0.367398967406893
Eval on 2487, saved at save/predictions.txt
Total 867460 tokens, accuracy: 45.42
  steps: 100  ppl: 5.2398  lr: 7.884974838245867e-05
  steps: 200  ppl: 3.5588  lr: 7.769949676491733e-05
  steps: 300  ppl: 3.3193  lr: 7.654924514737599e-05
  steps: 400  ppl: 3.1516  lr: 7.539899352983466e-05
  steps: 500  ppl: 2.9803  lr: 7.424874191229332e-05
  steps: 600  ppl: 2.902  lr: 7.309849029475199e-05
  steps: 700  ppl: 2.8695  lr: 7.194823867721066e-05
  steps: 800  ppl: 2.8076  lr: 7.079798705966932e-05
  steps: 900  ppl: 2.7484  lr: 6.964773544212798e-05
  steps: 1000  ppl: 2.6979  lr: 6.849748382458664e-05
0 are done!
5026, 0.6605650616792678
100 are done!
529928, 0.6277683005993268
Eval on 2487, saved at save/predictions.txt
Total 867460 tokens, accuracy: 73.46
Data size: 2487
Rank 0, load 0
Rank 0, load 10
Rank 0, load 20
Rank 0, load 30
Rank 0, load 40
Rank 0, load 50
Rank 0, load 60
Rank 0, load 70
Rank 0, load 80
Rank 0, load 90
Rank 0, load 100
tokens: 1496828
  perplexity = 2.4715
Saving model checkpoint to save/checkpoint-1000-2.4715
Saving optimizer and scheduler states to save/checkpoint-last
  steps: 1100  ppl: 2.6442  lr: 6.734723220704529e-05
  steps: 1200  ppl: 2.588  lr: 6.619698058950395e-05
  steps: 1300  ppl: 2.5969  lr: 6.504672897196261e-05
  steps: 1400  ppl: 2.6101  lr: 6.389647735442128e-05
  steps: 1500  ppl: 2.4825  lr: 6.274622573687994e-05
  steps: 1600  ppl: 2.516  lr: 6.15959741193386e-05
  steps: 1700  ppl: 2.4629  lr: 6.0445722501797276e-05
  steps: 1800  ppl: 2.5012  lr: 5.929547088425594e-05
  steps: 1900  ppl: 2.4406  lr: 5.81452192667146e-05
  steps: 2000  ppl: 2.4551  lr: 5.699496764917326e-05
0 are done!
5026, 0.6752884998010347
100 are done!
529928, 0.6402190486254736
Eval on 2487, saved at save/predictions.txt
Total 867460 tokens, accuracy: 74.72
  perplexity = 2.2974
Saving model checkpoint to save/checkpoint-2000-2.2974
Saving optimizer and scheduler states to save/checkpoint-last
  steps: 2100  ppl: 2.4261  lr: 5.5844716031631925e-05
  steps: 2200  ppl: 2.4139  lr: 5.469446441409059e-05
  steps: 2300  ppl: 2.3881  lr: 5.354421279654925e-05
  steps: 2400  ppl: 2.4145  lr: 5.239396117900791e-05
  steps: 2500  ppl: 2.3955  lr: 5.1243709561466573e-05
  steps: 2600  ppl: 2.3646  lr: 5.0093457943925236e-05
  steps: 2700  ppl: 2.3679  lr: 4.89432063263839e-05
  steps: 2800  ppl: 2.3491  lr: 4.779295470884257e-05
  steps: 2900  ppl: 2.3079  lr: 4.664270309130123e-05
  steps: 3000  ppl: 2.291  lr: 4.549245147375989e-05
0 are done!
5026, 0.6788698766414644
100 are done!
529928, 0.6466897389834091
Eval on 2487, saved at save/predictions.txt
Total 867460 tokens, accuracy: 75.46
  perplexity = 2.2128
Saving model checkpoint to save/checkpoint-3000-2.2128
Saving optimizer and scheduler states to save/checkpoint-last
  steps: 3100  ppl: 2.3005  lr: 4.434219985621855e-05
  steps: 3200  ppl: 2.2904  lr: 4.3191948238677215e-05
  steps: 3300  ppl: 2.2974  lr: 4.204169662113588e-05
  steps: 3400  ppl: 2.2294  lr: 4.089144500359454e-05
  steps: 3500  ppl: 2.2985  lr: 3.97411933860532e-05
  steps: 3600  ppl: 2.2676  lr: 3.8590941768511864e-05
  steps: 3700  ppl: 2.2784  lr: 3.7440690150970526e-05
  steps: 3800  ppl: 2.285  lr: 3.629043853342919e-05
  steps: 3900  ppl: 2.2536  lr: 3.514018691588785e-05
  steps: 4000  ppl: 2.2441  lr: 3.398993529834651e-05
0 are done!
5026, 0.6842419419021091
100 are done!
529928, 0.6521716157666702
Eval on 2487, saved at save/predictions.txt
Total 867460 tokens, accuracy: 76.05
  perplexity = 2.1603
Saving model checkpoint to save/checkpoint-4000-2.1603
Saving optimizer and scheduler states to save/checkpoint-last
  steps: 4100  ppl: 2.2296  lr: 3.2839683680805174e-05
  steps: 4200  ppl: 2.2621  lr: 3.168943206326384e-05
  steps: 4300  ppl: 2.2132  lr: 3.0539180445722505e-05
  steps: 4400  ppl: 2.231  lr: 2.9388928828181168e-05
  steps: 4500  ppl: 2.1745  lr: 2.823867721063983e-05
  steps: 4600  ppl: 2.2116  lr: 2.7088425593098492e-05
  steps: 4700  ppl: 2.1755  lr: 2.5938173975557157e-05
  steps: 4800  ppl: 2.1999  lr: 2.478792235801582e-05
  steps: 4900  ppl: 2.1704  lr: 2.363767074047448e-05
  steps: 5000  ppl: 2.2158  lr: 2.2487419122933144e-05
0 are done!
5026, 0.6834460803820135
100 are done!
529928, 0.6539567639377425
Eval on 2487, saved at save/predictions.txt
Total 867460 tokens, accuracy: 76.22
  perplexity = 2.132
Saving model checkpoint to save/checkpoint-5000-2.132
Saving optimizer and scheduler states to save/checkpoint-last
  steps: 5100  ppl: 2.1924  lr: 2.1337167505391806e-05
  steps: 5200  ppl: 2.1832  lr: 2.0186915887850468e-05
  steps: 5300  ppl: 2.1731  lr: 1.903666427030913e-05
  steps: 5400  ppl: 2.1953  lr: 1.7886412652767796e-05
  steps: 5500  ppl: 2.1667  lr: 1.6736161035226458e-05
  steps: 5600  ppl: 2.1607  lr: 1.558590941768512e-05
  steps: 5700  ppl: 2.1499  lr: 1.4435657800143784e-05
  steps: 5800  ppl: 2.1481  lr: 1.3285406182602444e-05
  steps: 5900  ppl: 2.1506  lr: 1.213515456506111e-05
  steps: 6000  ppl: 2.1692  lr: 1.0984902947519772e-05
0 are done!
5026, 0.6894150417827298
100 are done!
529928, 0.6558891019157319
Eval on 2487, saved at save/predictions.txt
Total 867460 tokens, accuracy: 76.44
  perplexity = 2.1121
Saving model checkpoint to save/checkpoint-6000-2.1121
Saving optimizer and scheduler states to save/checkpoint-last
  steps: 6100  ppl: 2.1689  lr: 9.834651329978434e-06
  steps: 6200  ppl: 2.1685  lr: 8.684399712437096e-06
  steps: 6300  ppl: 2.1398  lr: 7.534148094895759e-06
  steps: 6400  ppl: 2.1517  lr: 6.383896477354422e-06
  steps: 6500  ppl: 2.1533  lr: 5.233644859813084e-06
  steps: 6600  ppl: 2.1352  lr: 4.083393242271747e-06
  steps: 6700  ppl: 2.14  lr: 2.93314162473041e-06
  steps: 6800  ppl: 2.1348  lr: 1.7828900071890728e-06
  steps: 6900  ppl: 2.1413  lr: 6.326383896477354e-07
 global_step = 6955, average loss = 0.8713822233481942
Model has a total of 1415660931 trainable parameters
Training/evaluation parameters Namespace(data_dir='../../../datasets/kotlin', langs='kotlin', output_dir='save/', model_type='phi_1_5', pretrain_dir='microsoft/phi-1_5', config_dir=None, tokenizer_dir=None, lit_file='../../../datasets/kotlin/literals.json', load_name='pretrained', mlm=False, mlm_probability=0.15, cache_dir='', block_size=512, do_train=True, do_eval=False, evaluate_during_training=True, do_lower_case=False, per_gpu_train_batch_size=4, per_gpu_eval_batch_size=8, gradient_accumulation_steps=4, learning_rate=8e-05, weight_decay=0.01, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=5.0, max_steps=-1, warmup_steps=0, logging_steps=100, save_steps=1000, save_total_limit=None, eval_all_checkpoints=False, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=42, not_pretrain=True, fp16=False, fp16_opt_level='O1', local_rank=-1, node_index=-1, gpu_per_node=1, server_ip='', server_port='', log_file='completion_kotlin_eval_vanilla_gpt_py.log', tensorboard_dir=None, wandb_run_name='PHI_1_5_FINETUNE', wandb_project_name='JB_internship_2024', n_gpu=1, device=device(type='cuda'), start_epoch=0, start_step=0)
Creating features from dataset file at ../../../datasets/kotlin/train.txt
Data size: 39754
Rank 0, load 0
Rank 0, load 10
Rank 0, load 20
Rank 0, load 30
Rank 0, load 40
Rank 0, load 50
Rank 0, load 60
Rank 0, load 70
Rank 0, load 80
Rank 0, load 90
Rank 0, load 100
tokens: 22796815
Rank 0 Training 22796815 token, 44525 samples
Saving features into cached file save/train_blocksize_512_wordsize_1_rank_0
***** Running training *****
  Num examples = 44525
  Num epoch = 4
  Instantaneous batch size per GPU = 4
  Total train batch size (w. parallel, distributed & accumulation) = 16
  Gradient Accumulation steps = 4
  Total optimization steps = 13910
Data size: 2487
load 0
load 10
load 20
load 30
load 40
load 50
load 60
load 70
load 80
load 90
load 100
tokens: 1496828
0 are done!
2227, 0.5779074988774136
100 are done!
261277, 0.4837318248448964
200 are done!
526617, 0.4887100112605556
300 are done!
803069, 0.5260245383646984
Eval on 2487, saved at save/predictions.txt
Total 867460 tokens, accuracy: 58.05
Model has a total of 1415660931 trainable parameters
Training/evaluation parameters Namespace(data_dir='../../../datasets/kotlin', langs='kotlin', output_dir='save/', model_type='phi_1_5', pretrain_dir='microsoft/phi-1_5', config_dir=None, tokenizer_dir=None, lit_file='../../../datasets/kotlin/literals.json', load_name='pretrained', mlm=False, mlm_probability=0.15, cache_dir='', block_size=512, do_train=True, do_eval=False, evaluate_during_training=True, do_lower_case=False, per_gpu_train_batch_size=4, per_gpu_eval_batch_size=4, gradient_accumulation_steps=4, learning_rate=8e-05, weight_decay=0.01, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=5.0, max_steps=-1, warmup_steps=0, logging_steps=100, save_steps=1000, save_total_limit=None, eval_all_checkpoints=False, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=42, not_pretrain=True, fp16=False, fp16_opt_level='O1', local_rank=-1, node_index=-1, gpu_per_node=1, server_ip='', server_port='', log_file='completion_kotlin_eval_vanilla_gpt_py.log', tensorboard_dir=None, wandb_run_name='PHI_1_5_FINETUNE', wandb_project_name='JB_internship_2024', n_gpu=1, device=device(type='cuda'), start_epoch=0, start_step=0)
Loading features from cached file save/train_blocksize_512_wordsize_1_rank_0
***** Running training *****
  Num examples = 44525
  Num epoch = 4
  Instantaneous batch size per GPU = 4
  Total train batch size (w. parallel, distributed & accumulation) = 16
  Gradient Accumulation steps = 4
  Total optimization steps = 13910
0 are done!
1159, 0.5798101811906816
100 are done!
129844, 0.4881704198884816
200 are done!
260023, 0.48397257165712265
300 are done!
393050, 0.48858923801043125
400 are done!
525219, 0.48883608551861224
500 are done!
666311, 0.5357648305370916
600 are done!
801699, 0.5261014420624199
700 are done!
930580, 0.5186109738012852
Eval on 2487, saved at save/predictions.txt
Total 867460 tokens, accuracy: 58.05
Model has a total of 1415660931 trainable parameters
Training/evaluation parameters Namespace(data_dir='../../../datasets/kotlin', langs='kotlin', output_dir='save/', model_type='phi_1_5', pretrain_dir='microsoft/phi-1_5', config_dir=None, tokenizer_dir=None, lit_file='../../../datasets/kotlin/literals.json', load_name='pretrained', mlm=False, mlm_probability=0.15, cache_dir='', block_size=512, do_train=True, do_eval=False, evaluate_during_training=True, do_lower_case=False, per_gpu_train_batch_size=2, per_gpu_eval_batch_size=4, gradient_accumulation_steps=8, learning_rate=8e-05, weight_decay=0.01, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=5.0, max_steps=-1, warmup_steps=0, logging_steps=100, save_steps=1000, save_total_limit=None, eval_all_checkpoints=False, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=42, not_pretrain=True, fp16=False, fp16_opt_level='O1', local_rank=-1, node_index=-1, gpu_per_node=1, server_ip='', server_port='', log_file='completion_kotlin_eval_vanilla_gpt_py.log', tensorboard_dir=None, wandb_run_name='PHI_1_5_FINETUNE', wandb_project_name='JB_internship_2024', n_gpu=1, device=device(type='cuda'), start_epoch=0, start_step=0)
Loading features from cached file save/train_blocksize_512_wordsize_1_rank_0
***** Running training *****
  Num examples = 44525
  Num epoch = 4
  Instantaneous batch size per GPU = 2
  Total train batch size (w. parallel, distributed & accumulation) = 16
  Gradient Accumulation steps = 8
  Total optimization steps = 13910
0 are done!
1159, 0.5798101811906816
100 are done!
129844, 0.4881704198884816
200 are done!
260023, 0.48397257165712265
300 are done!
393050, 0.48858923801043125
400 are done!
525219, 0.48883608551861224
500 are done!
666311, 0.5357648305370916
600 are done!
801699, 0.5261014420624199
700 are done!
930580, 0.5186109738012852
Eval on 2487, saved at save/predictions.txt
Total 867460 tokens, accuracy: 58.05
Model has a total of 1415660931 trainable parameters
Training/evaluation parameters Namespace(data_dir='../../../datasets/kotlin', langs='kotlin', output_dir='save/', model_type='phi_1_5', pretrain_dir='microsoft/phi-1_5', config_dir=None, tokenizer_dir=None, lit_file='../../../datasets/kotlin/literals.json', load_name='pretrained', mlm=False, mlm_probability=0.15, cache_dir='', block_size=512, do_train=True, do_eval=False, evaluate_during_training=True, do_lower_case=False, per_gpu_train_batch_size=2, per_gpu_eval_batch_size=2, gradient_accumulation_steps=8, learning_rate=8e-05, weight_decay=0.01, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=5.0, max_steps=-1, warmup_steps=0, logging_steps=100, save_steps=1000, save_total_limit=None, eval_all_checkpoints=False, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=42, not_pretrain=True, fp16=False, fp16_opt_level='O1', local_rank=-1, node_index=-1, gpu_per_node=1, server_ip='', server_port='', log_file='completion_kotlin_eval_vanilla_gpt_py.log', tensorboard_dir=None, wandb_run_name='PHI_1_5_FINETUNE', wandb_project_name='JB_internship_2024', n_gpu=1, device=device(type='cuda'), start_epoch=0, start_step=0)
Loading features from cached file save/train_blocksize_512_wordsize_1_rank_0
***** Running training *****
  Num examples = 44525
  Num epoch = 4
  Instantaneous batch size per GPU = 2
  Total train batch size (w. parallel, distributed & accumulation) = 16
  Gradient Accumulation steps = 8
  Total optimization steps = 13910
0 are done!
643, 0.6189735614307932
100 are done!
64441, 0.48495523036576094
200 are done!
129199, 0.488548672977345
300 are done!
193258, 0.4834314750230262
400 are done!
259267, 0.4842536844257079
500 are done!
326182, 0.4901803287735068
600 are done!
392442, 0.48866329291971805
700 are done!
458569, 0.48660070785421605
800 are done!
524483, 0.48892719115776867
900 are done!
593484, 0.49830492481684424
1000 are done!
665579, 0.535729041931912
1100 are done!
732143, 0.5324765790289602
1200 are done!
800983, 0.5260361331014516
1300 are done!
865886, 0.5230619273206866
1400 are done!
930023, 0.5185559927012557
Eval on 2487, saved at save/predictions.txt
Total 867460 tokens, accuracy: 58.05
Model has a total of 1415660931 trainable parameters
Training/evaluation parameters Namespace(data_dir='../../../datasets/kotlin', langs='kotlin', output_dir='save/', model_type='phi_1_5', pretrain_dir='microsoft/phi-1_5', config_dir=None, tokenizer_dir=None, lit_file='../../../datasets/kotlin/literals.json', load_name='pretrained', mlm=False, mlm_probability=0.15, cache_dir='', block_size=512, do_train=True, do_eval=False, evaluate_during_training=True, do_lower_case=False, per_gpu_train_batch_size=2, per_gpu_eval_batch_size=2, gradient_accumulation_steps=8, learning_rate=8e-05, weight_decay=0.01, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=5.0, max_steps=-1, warmup_steps=0, logging_steps=100, save_steps=1000, save_total_limit=None, eval_all_checkpoints=False, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=42, not_pretrain=True, fp16=False, fp16_opt_level='O1', local_rank=-1, node_index=-1, gpu_per_node=1, server_ip='', server_port='', log_file='completion_kotlin_eval_vanilla_gpt_py.log', tensorboard_dir=None, wandb_run_name='PHI_1_5_FINETUNE', wandb_project_name='JB_internship_2024', n_gpu=1, device=device(type='cuda'), start_epoch=0, start_step=0)
Creating features from dataset file at ../../../datasets/kotlin/train.txt
Model has a total of 1415660931 trainable parameters
Training/evaluation parameters Namespace(data_dir='../../../datasets/kotlin', langs='kotlin', output_dir='save/', model_type='phi_1_5', pretrain_dir='microsoft/phi-1_5', config_dir=None, tokenizer_dir=None, lit_file='../../../datasets/kotlin/literals.json', load_name='pretrained', mlm=False, mlm_probability=0.15, cache_dir='', block_size=512, do_train=True, do_eval=False, evaluate_during_training=True, do_lower_case=False, per_gpu_train_batch_size=2, per_gpu_eval_batch_size=2, gradient_accumulation_steps=8, learning_rate=8e-05, weight_decay=0.01, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=5.0, max_steps=-1, warmup_steps=0, logging_steps=100, save_steps=1000, save_total_limit=None, eval_all_checkpoints=False, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=42, not_pretrain=True, fp16=False, fp16_opt_level='O1', local_rank=-1, node_index=-1, gpu_per_node=1, server_ip='', server_port='', log_file='completion_kotlin_eval_vanilla_gpt_py.log', tensorboard_dir=None, wandb_run_name='PHI_1_5_FINETUNE', wandb_project_name='JB_internship_2024', n_gpu=1, device=device(type='cuda'), start_epoch=0, start_step=0)
Creating features from dataset file at ../../../datasets/kotlin/train.txt
Data size: 39754
Rank 0, load 0
Rank 0, load 10
Rank 0, load 20
Rank 0, load 30
Rank 0, load 40
Rank 0, load 50
Rank 0, load 60
Rank 0, load 70
Rank 0, load 80
Rank 0, load 90
Rank 0, load 100
tokens: 22796815
Rank 0 Training 22796815 token, 44525 samples
Saving features into cached file save/train_blocksize_512_wordsize_1_rank_0
***** Running training *****
  Num examples = 44525
  Num epoch = 4
  Instantaneous batch size per GPU = 2
  Total train batch size (w. parallel, distributed & accumulation) = 16
  Gradient Accumulation steps = 8
  Total optimization steps = 13910
Data size: 2487
load 0
load 10
load 20
load 30
load 40
load 50
load 60
load 70
load 80
load 90
load 100
tokens: 1496828
0 are done!
643, 0.6189735614307932
100 are done!
64441, 0.48495523036576094
200 are done!
129199, 0.488548672977345
300 are done!
193258, 0.4834314750230262
400 are done!
259267, 0.4842536844257079
500 are done!
326182, 0.4901803287735068
600 are done!
392442, 0.48866329291971805
700 are done!
458569, 0.48660070785421605
800 are done!
524483, 0.48892719115776867
900 are done!
593484, 0.49830492481684424
1000 are done!
665579, 0.535729041931912
1100 are done!
732143, 0.5324765790289602
1200 are done!
800983, 0.5260361331014516
1300 are done!
865886, 0.5230619273206866
1400 are done!
930023, 0.5185559927012557
Eval on 2487, saved at save/predictions.txt
Total 867460 tokens, accuracy: 58.05
  steps: 100  ppl: 4.0035  lr: 7.942487419122934e-05
  steps: 200  ppl: 2.649  lr: 7.884974838245867e-05
  steps: 300  ppl: 2.4827  lr: 7.8274622573688e-05
  steps: 400  ppl: 2.3726  lr: 7.769949676491733e-05
  steps: 500  ppl: 2.3149  lr: 7.712437095614666e-05
  steps: 600  ppl: 2.2194  lr: 7.654924514737599e-05
  steps: 700  ppl: 2.1958  lr: 7.597411933860532e-05
  steps: 800  ppl: 2.1401  lr: 7.539899352983466e-05
  steps: 900  ppl: 2.1128  lr: 7.482386772106399e-05
  steps: 1000  ppl: 2.0927  lr: 7.424874191229332e-05
0 are done!
643, 0.7216174183514774
100 are done!
64441, 0.6667804658524852
200 are done!
129199, 0.6681166262896772
300 are done!
193258, 0.6593931428453156
400 are done!
259267, 0.6572375196226284
500 are done!
326182, 0.6621058182241816
600 are done!
392442, 0.6604033207454859
700 are done!
458569, 0.657938063846444
800 are done!
524483, 0.6603207348951253
900 are done!
593484, 0.6680550781486948
1000 are done!
665579, 0.6958497789142987
1100 are done!
732143, 0.6938617182708843
1200 are done!
800983, 0.690213150591211
1300 are done!
865886, 0.6891415267136782
1400 are done!
930023, 0.6854991758268344
Eval on 2487, saved at save/predictions.txt
Total 867460 tokens, accuracy: 76.8
Data size: 2487
Rank 0, load 0
Rank 0, load 10
Rank 0, load 20
Rank 0, load 30
Rank 0, load 40
Rank 0, load 50
Rank 0, load 60
Rank 0, load 70
Rank 0, load 80
Rank 0, load 90
Rank 0, load 100
tokens: 1496828
  perplexity = 2.0733
Saving model checkpoint to save/checkpoint-1000-2.0733
Saving optimizer and scheduler states to save/checkpoint-last
  steps: 1100  ppl: 2.0442  lr: 7.367361610352265e-05
  steps: 1200  ppl: 2.0369  lr: 7.309849029475199e-05
  steps: 1300  ppl: 2.0246  lr: 7.252336448598131e-05
  steps: 1400  ppl: 2.0041  lr: 7.194823867721066e-05
  steps: 1500  ppl: 1.9685  lr: 7.137311286843997e-05
  steps: 1600  ppl: 1.9813  lr: 7.079798705966932e-05
  steps: 1700  ppl: 1.9629  lr: 7.022286125089863e-05
  steps: 1800  ppl: 1.9464  lr: 6.964773544212798e-05
  steps: 1900  ppl: 1.9498  lr: 6.90726096333573e-05
  steps: 2000  ppl: 1.9106  lr: 6.849748382458664e-05
0 are done!
643, 0.7480559875583204
100 are done!
64441, 0.6793501031951708
200 are done!
129199, 0.6814681228182881
300 are done!
193258, 0.6733175340736218
400 are done!
259267, 0.6706831181754717
500 are done!
326182, 0.6750127229583484
600 are done!
392442, 0.6736893604660051
700 are done!
458569, 0.6712054238293473
800 are done!
524483, 0.6733278295006702
900 are done!
593484, 0.6807327577491559
1000 are done!
665579, 0.7073810922520092
1100 are done!
732143, 0.7055807403744897
1200 are done!
800983, 0.702054850102936
1300 are done!
865886, 0.7009259879476051
1400 are done!
930023, 0.6972494228637356
Eval on 2487, saved at save/predictions.txt
Total 867460 tokens, accuracy: 78.14
  perplexity = 1.9285
Saving model checkpoint to save/checkpoint-2000-1.9285
Saving optimizer and scheduler states to save/checkpoint-last
  steps: 2100  ppl: 1.8803  lr: 6.792235801581596e-05
  steps: 2200  ppl: 1.8787  lr: 6.734723220704529e-05
  steps: 2300  ppl: 1.8741  lr: 6.677210639827463e-05
  steps: 2400  ppl: 1.8551  lr: 6.619698058950395e-05
  steps: 2500  ppl: 1.8568  lr: 6.56218547807333e-05
  steps: 2600  ppl: 1.8317  lr: 6.504672897196261e-05
  steps: 2700  ppl: 1.8403  lr: 6.447160316319196e-05
  steps: 2800  ppl: 1.8039  lr: 6.389647735442128e-05
  steps: 2900  ppl: 1.5767  lr: 6.332135154565062e-05
  steps: 3000  ppl: 1.5675  lr: 6.274622573687994e-05
0 are done!
643, 0.7527216174183515
100 are done!
64441, 0.6903989696000993
200 are done!
129199, 0.6908257803852971
300 are done!
193258, 0.6811723188690766
400 are done!
259267, 0.6786401663150343
500 are done!
326182, 0.6827108792024085
600 are done!
392442, 0.6810152837871584
700 are done!
458569, 0.6790166801506425
800 are done!
524483, 0.6813681282329456
900 are done!
593484, 0.6884583240660237
1000 are done!
665579, 0.7142863581933925
1100 are done!
732143, 0.7124141048948088
1200 are done!
800983, 0.7086929435456183
1300 are done!
865886, 0.7077375081708216
1400 are done!
930023, 0.7042890337120695
Eval on 2487, saved at save/predictions.txt
Total 867460 tokens, accuracy: 78.94
  perplexity = 1.8919
Saving model checkpoint to save/checkpoint-3000-1.8919
Saving optimizer and scheduler states to save/checkpoint-last
  steps: 3100  ppl: 1.5839  lr: 6.217109992810928e-05
  steps: 3200  ppl: 1.5997  lr: 6.15959741193386e-05
  steps: 3300  ppl: 1.5855  lr: 6.102084831056794e-05
  steps: 3400  ppl: 1.5745  lr: 6.0445722501797276e-05
  steps: 3500  ppl: 1.5857  lr: 5.98705966930266e-05
  steps: 3600  ppl: 1.5798  lr: 5.929547088425594e-05
  steps: 3700  ppl: 1.5825  lr: 5.872034507548526e-05
  steps: 3800  ppl: 1.5891  lr: 5.81452192667146e-05
  steps: 3900  ppl: 1.5796  lr: 5.7570093457943925e-05
  steps: 4000  ppl: 1.5831  lr: 5.699496764917326e-05
0 are done!
643, 0.7729393468118196
100 are done!
64441, 0.69735106531556
200 are done!
129199, 0.6979233585399268
300 are done!
193258, 0.6872212275817818
400 are done!
259267, 0.6847689833260693
500 are done!
326182, 0.6891735288887799
600 are done!
392442, 0.6876481110584494
700 are done!
458569, 0.6856917933833294
800 are done!
524483, 0.6876485987153063
900 are done!
593484, 0.6949673453707261
1000 are done!
665579, 0.7207754451387438
1100 are done!
732143, 0.7189947865376026
1200 are done!
800983, 0.7152136811892387
1300 are done!
865886, 0.7141586767773125
1400 are done!
930023, 0.7107329603676468
Eval on 2487, saved at save/predictions.txt
Total 867460 tokens, accuracy: 79.66
  perplexity = 1.8479
Saving model checkpoint to save/checkpoint-4000-1.8479
Saving optimizer and scheduler states to save/checkpoint-last
  steps: 4100  ppl: 1.572  lr: 5.641984184040259e-05
  steps: 4200  ppl: 1.5674  lr: 5.5844716031631925e-05
  steps: 4300  ppl: 1.5652  lr: 5.526959022286125e-05
  steps: 4400  ppl: 1.5536  lr: 5.469446441409059e-05
  steps: 4500  ppl: 1.5499  lr: 5.4119338605319925e-05
  steps: 4600  ppl: 1.5688  lr: 5.354421279654925e-05
  steps: 4700  ppl: 1.5682  lr: 5.296908698777859e-05
  steps: 4800  ppl: 1.5551  lr: 5.239396117900791e-05
  steps: 4900  ppl: 1.5624  lr: 5.181883537023725e-05
  steps: 5000  ppl: 1.5416  lr: 5.1243709561466573e-05
0 are done!
643, 0.7636080870917574
100 are done!
64441, 0.7008115951024969
200 are done!
129199, 0.7023351573928591
300 are done!
193258, 0.6925094950791171
400 are done!
259267, 0.6907782324784875
500 are done!
326182, 0.6948360117970949
600 are done!
392442, 0.693274420169095
700 are done!
458569, 0.6912460284057579
800 are done!
524483, 0.693265558654904
900 are done!
593484, 0.7002985758672516
1000 are done!
665579, 0.7253353846801056
1100 are done!
732143, 0.7235662978407224
1200 are done!
800983, 0.7199129070155047
1300 are done!
865886, 0.7190045802796211
1400 are done!
930023, 0.715694127994684
Eval on 2487, saved at save/predictions.txt
Total 867460 tokens, accuracy: 80.22
  perplexity = 1.8095
Saving model checkpoint to save/checkpoint-5000-1.8095
Saving optimizer and scheduler states to save/checkpoint-last
  steps: 5100  ppl: 1.5425  lr: 5.0668583752695905e-05
  steps: 5200  ppl: 1.5395  lr: 5.0093457943925236e-05
  steps: 5300  ppl: 1.5329  lr: 4.951833213515457e-05
  steps: 5400  ppl: 1.5389  lr: 4.89432063263839e-05
  steps: 5500  ppl: 1.5304  lr: 4.836808051761323e-05
  steps: 5600  ppl: 1.4483  lr: 4.779295470884257e-05
  steps: 5700  ppl: 1.3012  lr: 4.721782890007189e-05
  steps: 5800  ppl: 1.2936  lr: 4.664270309130123e-05
  steps: 5900  ppl: 1.2917  lr: 4.606757728253055e-05
  steps: 6000  ppl: 1.2993  lr: 4.549245147375989e-05
0 are done!
643, 0.76049766718507
100 are done!
64441, 0.7042410887478469
200 are done!
129199, 0.7052840966261349
300 are done!
193258, 0.6950760123772366
400 are done!
259267, 0.6926218917178044
500 are done!
326182, 0.6964884634958397
600 are done!
392442, 0.6947982122198949
700 are done!
458569, 0.6930341998695944
800 are done!
524483, 0.6951531317506955
900 are done!
593484, 0.7022817801322361
1000 are done!
665579, 0.7269069486867825
1100 are done!
732143, 0.7251670780161799
1200 are done!
800983, 0.7215396581450543
1300 are done!
865886, 0.7206468287973243
1400 are done!
930023, 0.7172736588234915
Eval on 2487, saved at save/predictions.txt
Total 867460 tokens, accuracy: 80.39
  perplexity = 1.891
Saving model checkpoint to save/checkpoint-6000-1.891
Saving optimizer and scheduler states to save/checkpoint-last
  steps: 6100  ppl: 1.3012  lr: 4.4917325664989215e-05
