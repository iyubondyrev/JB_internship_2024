[50295, 23748, 995, 50298, 50563, 50299, 220, 50296]
PhiConfig {
  "_name_or_path": "iyubondyrev/jb_2024_kotlin_phi-1_5",
  "architectures": [
    "PhiForCausalLM"
  ],
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "microsoft/phi-1_5--configuration_phi.PhiConfig",
    "AutoModelForCausalLM": "microsoft/phi-1_5--modeling_phi.PhiForCausalLM"
  },
  "bos_token_id": 50295,
  "embd_pdrop": 0.0,
  "eos_token_id": 50296,
  "hidden_act": "gelu_new",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 2048,
  "model_type": "phi",
  "num_attention_heads": 32,
  "num_hidden_layers": 24,
  "num_key_value_heads": 32,
  "pad_token_id": 50299,
  "partial_rotary_factor": 0.5,
  "qk_layernorm": false,
  "resid_pdrop": 0.0,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.40.1",
  "use_cache": true,
  "vocab_size": 50565
}

Model has a total of 1415669125 trainable parameters
Training/evaluation parameters Namespace(data_dir='../../../datasets/method_generation_dataset/kotlin', output_dir='save_phi/', model_type='phi_1_5', pretrain_dir='iyubondyrev/jb_2024_kotlin_phi-1_5', config_dir=None, tokenizer_dir=None, load_name='pretrained', lit_file='../../../datasets/method_generation_dataset/kotlin/literals.json', mlm=False, mlm_probability=0.15, cache_dir='', block_size=512, do_train=True, do_eval=False, do_infer=False, evaluate_during_training=True, do_lower_case=False, per_gpu_train_batch_size=4, per_gpu_eval_batch_size=12, gradient_accumulation_steps=1, learning_rate=3e-05, weight_decay=0.01, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=6.0, max_steps=-1, warmup_steps=1000, logging_steps=100, save_steps=1000, save_total_limit=None, eval_all_checkpoints=False, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, node_index=0, gpu_per_node=1, server_ip='', server_port='', log_file='method_gen_kotlin_train_my_phi-1_5.log', tensorboard_dir=None, lang='kotlin', n_gpu=1, device=device(type='cuda'), start_step=0)
Creating features from dataset file at ../../../datasets/method_generation_dataset/kotlin/train.jsonl
Data size: 10597
Rank 0, load 0
Rank 0, load 10
Rank 0, load 20
Rank 0, load 30
Rank 0, load 40
Rank 0, load 50
Rank 0, load 60
Rank 0, load 70
Rank 0, load 80
Rank 0, load 90
Rank 0, load 100
Rank 0 Training 10597 samples
Saving features into cached file save_phi/train_blocksize_512_wordsize_1_rank_0
***** Running training *****
  Num examples = 10597
  Num epoch = 5
  Instantaneous batch size per GPU = 4
  Total train batch size (w. parallel, distributed & accumulation) = 4
  Gradient Accumulation steps = 1
  Total optimization steps = 15894
  steps: 100  lr: 3e-06  loss: 1.8067
  steps: 200  lr: 6e-06  loss: 1.3246
  steps: 300  lr: 9e-06  loss: 1.302
  steps: 400  lr: 1.2e-05  loss: 1.3157
  steps: 500  lr: 1.5e-05  loss: 1.3459
  steps: 600  lr: 1.8e-05  loss: 1.3651
  steps: 700  lr: 2.1e-05  loss: 1.3563
  steps: 800  lr: 2.4e-05  loss: 1.3017
  steps: 900  lr: 2.7e-05  loss: 1.32
  steps: 1000  lr: 3e-05  loss: 1.3898
Data size: 669
Rank 0, load 0
Rank 0, load 10
Rank 0, load 20
Rank 0, load 30
Rank 0, load 40
Rank 0, load 50
Rank 0, load 60
Rank 0, load 70
Rank 0, load 80
Rank 0, load 90
Rank 0, load 100
Eval steps: 0
  perplexity = 1.7225
Saving model checkpoint to save_phi/checkpoint-1000-1.7225
Saving optimizer states to save_phi/checkpoint-last
  steps: 1100  lr: 3e-05  loss: 1.4023
  steps: 1200  lr: 3e-05  loss: 1.3966
  steps: 1300  lr: 2.9e-05  loss: 1.4224
  steps: 1400  lr: 2.9e-05  loss: 1.3968
  steps: 1500  lr: 2.9e-05  loss: 1.4413
  steps: 1600  lr: 2.9e-05  loss: 1.4276
  steps: 1700  lr: 2.9e-05  loss: 1.374
  steps: 1800  lr: 2.8e-05  loss: 1.3851
  steps: 1900  lr: 2.8e-05  loss: 1.4289
  steps: 2000  lr: 2.8e-05  loss: 1.3588
Eval steps: 0
  perplexity = 1.7095
Saving model checkpoint to save_phi/checkpoint-2000-1.7095
Saving optimizer states to save_phi/checkpoint-last
  steps: 2100  lr: 2.8e-05  loss: 1.4101
  steps: 2200  lr: 2.8e-05  loss: 1.3996
  steps: 2300  lr: 2.7e-05  loss: 1.3771
  steps: 2400  lr: 2.7e-05  loss: 1.3532
  steps: 2500  lr: 2.7e-05  loss: 1.3662
  steps: 2600  lr: 2.7e-05  loss: 1.4156
  steps: 2700  lr: 2.7e-05  loss: 1.2867
  steps: 2800  lr: 2.6e-05  loss: 1.1846
  steps: 2900  lr: 2.6e-05  loss: 1.1667
  steps: 3000  lr: 2.6e-05  loss: 1.181
Eval steps: 0
  perplexity = 1.7403
Saving model checkpoint to save_phi/checkpoint-3000-1.7403
Saving optimizer states to save_phi/checkpoint-last
  steps: 3100  lr: 2.6e-05  loss: 1.1809
  steps: 3200  lr: 2.6e-05  loss: 1.1599
  steps: 3300  lr: 2.5e-05  loss: 1.162
  steps: 3400  lr: 2.5e-05  loss: 1.1822
  steps: 3500  lr: 2.5e-05  loss: 1.1835
  steps: 3600  lr: 2.5e-05  loss: 1.1662
  steps: 3700  lr: 2.5e-05  loss: 1.1819
  steps: 3800  lr: 2.4e-05  loss: 1.1863
  steps: 3900  lr: 2.4e-05  loss: 1.1659
  steps: 4000  lr: 2.4e-05  loss: 1.194
Eval steps: 0
  perplexity = 1.7497
Saving model checkpoint to save_phi/checkpoint-4000-1.7497
Saving optimizer states to save_phi/checkpoint-last
  steps: 4100  lr: 2.4e-05  loss: 1.1746
  steps: 4200  lr: 2.4e-05  loss: 1.165
  steps: 4300  lr: 2.3e-05  loss: 1.1796
  steps: 4400  lr: 2.3e-05  loss: 1.1626
  steps: 4500  lr: 2.3e-05  loss: 1.1675
  steps: 4600  lr: 2.3e-05  loss: 1.1845
  steps: 4700  lr: 2.3e-05  loss: 1.164
  steps: 4800  lr: 2.2e-05  loss: 1.1749
  steps: 4900  lr: 2.2e-05  loss: 1.1806
  steps: 5000  lr: 2.2e-05  loss: 1.1788
Eval steps: 0
  perplexity = 1.721
Saving model checkpoint to save_phi/checkpoint-5000-1.721
Saving optimizer states to save_phi/checkpoint-last
  steps: 5100  lr: 2.2e-05  loss: 1.1723
  steps: 5200  lr: 2.2e-05  loss: 1.172
  steps: 5300  lr: 2.1e-05  loss: 1.1712
  steps: 5400  lr: 2.1e-05  loss: 1.078
  steps: 5500  lr: 2.1e-05  loss: 1.0898
  steps: 5600  lr: 2.1e-05  loss: 1.0784
  steps: 5700  lr: 2.1e-05  loss: 1.0876
  steps: 5800  lr: 2e-05  loss: 1.0917
  steps: 5900  lr: 2e-05  loss: 1.0854
  steps: 6000  lr: 2e-05  loss: 1.0703
Eval steps: 0
  perplexity = 1.8051
Saving model checkpoint to save_phi/checkpoint-6000-1.8051
Saving optimizer states to save_phi/checkpoint-last
  steps: 6100  lr: 2e-05  loss: 1.0893
  steps: 6200  lr: 2e-05  loss: 1.0782
  steps: 6300  lr: 1.9e-05  loss: 1.0762
  steps: 6400  lr: 1.9e-05  loss: 1.0776
  steps: 6500  lr: 1.9e-05  loss: 1.0947
  steps: 6600  lr: 1.9e-05  loss: 1.0726
  steps: 6700  lr: 1.9e-05  loss: 1.0859
  steps: 6800  lr: 1.8e-05  loss: 1.0705
  steps: 6900  lr: 1.8e-05  loss: 1.0752
  steps: 7000  lr: 1.8e-05  loss: 1.0854
Eval steps: 0
  perplexity = 1.8031
Saving model checkpoint to save_phi/checkpoint-7000-1.8031
Saving optimizer states to save_phi/checkpoint-last
  steps: 7100  lr: 1.8e-05  loss: 1.0771
  steps: 7200  lr: 1.8e-05  loss: 1.0853
  steps: 7300  lr: 1.7e-05  loss: 1.0785
  steps: 7400  lr: 1.7e-05  loss: 1.0729
  steps: 7500  lr: 1.7e-05  loss: 1.0786
  steps: 7600  lr: 1.7e-05  loss: 1.067
  steps: 7700  lr: 1.7e-05  loss: 1.0783
  steps: 7800  lr: 1.6e-05  loss: 1.0767
  steps: 7900  lr: 1.6e-05  loss: 1.0706
  steps: 8000  lr: 1.6e-05  loss: 1.0485
Eval steps: 0
  perplexity = 1.8656
Saving model checkpoint to save_phi/checkpoint-8000-1.8656
Saving optimizer states to save_phi/checkpoint-last
  steps: 8100  lr: 1.6e-05  loss: 1.0419
  steps: 8200  lr: 1.5e-05  loss: 1.0421
  steps: 8300  lr: 1.5e-05  loss: 1.0473
  steps: 8400  lr: 1.5e-05  loss: 1.0394
  steps: 8500  lr: 1.5e-05  loss: 1.0442
  steps: 8600  lr: 1.5e-05  loss: 1.0433
  steps: 8700  lr: 1.4e-05  loss: 1.0372
  steps: 8800  lr: 1.4e-05  loss: 1.0427
  steps: 8900  lr: 1.4e-05  loss: 1.0366
  steps: 9000  lr: 1.4e-05  loss: 1.0341
Eval steps: 0
  perplexity = 1.8653
Saving model checkpoint to save_phi/checkpoint-9000-1.8653
Saving optimizer states to save_phi/checkpoint-last
  steps: 9100  lr: 1.4e-05  loss: 1.0364
  steps: 9200  lr: 1.3e-05  loss: 1.0428
  steps: 9300  lr: 1.3e-05  loss: 1.0409
  steps: 9400  lr: 1.3e-05  loss: 1.0313
  steps: 9500  lr: 1.3e-05  loss: 1.0337
  steps: 9600  lr: 1.3e-05  loss: 1.0355
  steps: 9700  lr: 1.2e-05  loss: 1.0405
  steps: 9800  lr: 1.2e-05  loss: 1.0333
  steps: 9900  lr: 1.2e-05  loss: 1.0332
  steps: 10000  lr: 1.2e-05  loss: 1.0367
Eval steps: 0
  perplexity = 1.8477
Saving model checkpoint to save_phi/checkpoint-10000-1.8477
Saving optimizer states to save_phi/checkpoint-last
  steps: 10100  lr: 1.2e-05  loss: 1.04
  steps: 10200  lr: 1.1e-05  loss: 1.0369
  steps: 10300  lr: 1.1e-05  loss: 1.0294
  steps: 10400  lr: 1.1e-05  loss: 1.0373
  steps: 10500  lr: 1.1e-05  loss: 1.0314
  steps: 10600  lr: 1.1e-05  loss: 1.0315
  steps: 10700  lr: 1e-05  loss: 1.0153
  steps: 10800  lr: 1e-05  loss: 1.0202
  steps: 10900  lr: 1e-05  loss: 1.0196
  steps: 11000  lr: 1e-05  loss: 1.0159
Eval steps: 0
  perplexity = 1.911
Saving model checkpoint to save_phi/checkpoint-11000-1.911
Saving optimizer states to save_phi/checkpoint-last
  steps: 11100  lr: 1e-05  loss: 1.0187
  steps: 11200  lr: 9e-06  loss: 1.0163
  steps: 11300  lr: 9e-06  loss: 1.017
  steps: 11400  lr: 9e-06  loss: 1.0173
  steps: 11500  lr: 9e-06  loss: 1.0129
  steps: 11600  lr: 9e-06  loss: 1.0174
  steps: 11700  lr: 8e-06  loss: 1.0152
  steps: 11800  lr: 8e-06  loss: 1.0138
  steps: 11900  lr: 8e-06  loss: 1.0159
  steps: 12000  lr: 8e-06  loss: 1.0219
Eval steps: 0
  perplexity = 1.9079
Saving model checkpoint to save_phi/checkpoint-12000-1.9079
Saving optimizer states to save_phi/checkpoint-last
  steps: 12100  lr: 8e-06  loss: 1.0142
  steps: 12200  lr: 7e-06  loss: 1.0162
  steps: 12300  lr: 7e-06  loss: 1.0155
  steps: 12400  lr: 7e-06  loss: 1.0135
  steps: 12500  lr: 7e-06  loss: 1.0172
  steps: 12600  lr: 7e-06  loss: 1.0185
  steps: 12700  lr: 6e-06  loss: 1.0175
  steps: 12800  lr: 6e-06  loss: 1.0125
  steps: 12900  lr: 6e-06  loss: 1.0154
  steps: 13000  lr: 6e-06  loss: 1.0141
Eval steps: 0
  perplexity = 1.9138
Saving model checkpoint to save_phi/checkpoint-13000-1.9138
Saving optimizer states to save_phi/checkpoint-last
  steps: 13100  lr: 6e-06  loss: 1.015
  steps: 13200  lr: 5e-06  loss: 1.014
  steps: 13300  lr: 5e-06  loss: 1.0097
  steps: 13400  lr: 5e-06  loss: 1.0063
  steps: 13500  lr: 5e-06  loss: 1.009
  steps: 13600  lr: 5e-06  loss: 1.0093
  steps: 13700  lr: 4e-06  loss: 1.0061
  steps: 13800  lr: 4e-06  loss: 1.0078
  steps: 13900  lr: 4e-06  loss: 1.0075
  steps: 14000  lr: 4e-06  loss: 1.0073
Eval steps: 0
  perplexity = 1.9721
Saving model checkpoint to save_phi/checkpoint-14000-1.9721
Saving optimizer states to save_phi/checkpoint-last
  steps: 14100  lr: 4e-06  loss: 1.0052
  steps: 14200  lr: 3e-06  loss: 1.0057
  steps: 14300  lr: 3e-06  loss: 1.0077
  steps: 14400  lr: 3e-06  loss: 1.0078
  steps: 14500  lr: 3e-06  loss: 1.0062
  steps: 14600  lr: 3e-06  loss: 1.0083
  steps: 14700  lr: 2e-06  loss: 1.0075
  steps: 14800  lr: 2e-06  loss: 1.0083
  steps: 14900  lr: 2e-06  loss: 1.0079
  steps: 15000  lr: 2e-06  loss: 1.0068
Eval steps: 0
  perplexity = 1.9929
Saving model checkpoint to save_phi/checkpoint-15000-1.9929
Saving optimizer states to save_phi/checkpoint-last
  steps: 15100  lr: 2e-06  loss: 1.0046
  steps: 15200  lr: 1e-06  loss: 1.0072
  steps: 15300  lr: 1e-06  loss: 1.007
  steps: 15400  lr: 1e-06  loss: 1.0075
  steps: 15500  lr: 1e-06  loss: 1.0051
  steps: 15600  lr: 1e-06  loss: 1.0055
  steps: 15700  lr: 0.0  loss: 1.0067
  steps: 15800  lr: 0.0  loss: 1.0047
 global_step = 15894, average loss = 0.10436845530627595
 [50295, 23748, 995, 50298, 50563, 50299, 220, 50296]
PhiConfig {
  "_name_or_path": "iyubondyrev/jb_2024_kotlin_phi-1_5",
  "architectures": [
    "PhiForCausalLM"
  ],
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "microsoft/phi-1_5--configuration_phi.PhiConfig",
    "AutoModelForCausalLM": "microsoft/phi-1_5--modeling_phi.PhiForCausalLM"
  },
  "bos_token_id": 50295,
  "embd_pdrop": 0.0,
  "eos_token_id": 50296,
  "hidden_act": "gelu_new",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 2048,
  "model_type": "phi",
  "num_attention_heads": 32,
  "num_hidden_layers": 24,
  "num_key_value_heads": 32,
  "pad_token_id": 50299,
  "partial_rotary_factor": 0.5,
  "qk_layernorm": false,
  "resid_pdrop": 0.0,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.41.0",
  "use_cache": true,
  "vocab_size": 50565
}

Model has a total of 103607685 trainable parameters
Training/evaluation parameters Namespace(data_dir='../../../datasets/method_generation_dataset/kotlin', output_dir='save_phi/', model_type='phi_1_5', pretrain_dir='iyubondyrev/jb_2024_kotlin_phi-1_5', config_dir=None, tokenizer_dir=None, load_name='pretrained', lit_file='../../../datasets/method_generation_dataset/kotlin/literals.json', mlm=False, mlm_probability=0.15, cache_dir='', block_size=512, do_train=True, do_eval=False, do_infer=False, evaluate_during_training=True, do_lower_case=False, per_gpu_train_batch_size=4, per_gpu_eval_batch_size=12, gradient_accumulation_steps=1, learning_rate=3e-05, weight_decay=0.01, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=6.0, max_steps=-1, warmup_steps=1000, logging_steps=100, save_steps=1000, save_total_limit=None, eval_all_checkpoints=False, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, node_index=0, gpu_per_node=1, server_ip='', server_port='', log_file='method_gen_kotlin_train_my_phi-1_5.log', tensorboard_dir=None, lang='kotlin', n_gpu=1, device=device(type='cuda'), start_step=0)
Creating features from dataset file at ../../../datasets/method_generation_dataset/kotlin/train.jsonl
Data size: 10597
Rank 0, load 0
Rank 0, load 10
Rank 0, load 20
Rank 0, load 30
Rank 0, load 40
Rank 0, load 50
Rank 0, load 60
Rank 0, load 70
Rank 0, load 80
Rank 0, load 90
Rank 0, load 100
Rank 0 Training 10597 samples
Saving features into cached file save_phi/train_blocksize_512_wordsize_1_rank_0
***** Running training *****
  Num examples = 10597
  Num epoch = 5
  Instantaneous batch size per GPU = 4
  Total train batch size (w. parallel, distributed & accumulation) = 4
  Gradient Accumulation steps = 1
  Total optimization steps = 15894
  steps: 100  lr: 3e-06  loss: 2.3652
  steps: 200  lr: 6e-06  loss: 2.2836
  steps: 300  lr: 9e-06  loss: 2.444
  steps: 400  lr: 1.2e-05  loss: 2.2829
[50295, 23748, 995, 50298, 50563, 50299, 220, 50296]
PhiConfig {
  "_name_or_path": "iyubondyrev/jb_2024_kotlin_phi-1_5",
  "architectures": [
    "PhiForCausalLM"
  ],
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "microsoft/phi-1_5--configuration_phi.PhiConfig",
    "AutoModelForCausalLM": "microsoft/phi-1_5--modeling_phi.PhiForCausalLM"
  },
  "bos_token_id": 50295,
  "embd_pdrop": 0.0,
  "eos_token_id": 50296,
  "hidden_act": "gelu_new",
  "hidden_size": 2048,
  "initializer_range": 0.02,
  "intermediate_size": 8192,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 2048,
  "model_type": "phi",
  "num_attention_heads": 32,
  "num_hidden_layers": 24,
  "num_key_value_heads": 32,
  "pad_token_id": 50299,
  "partial_rotary_factor": 0.5,
  "qk_layernorm": false,
  "resid_pdrop": 0.0,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.41.0",
  "use_cache": true,
  "vocab_size": 50565
}

Model has a total of 103607685 trainable parameters
Training/evaluation parameters Namespace(data_dir='../../../datasets/method_generation_dataset/kotlin', output_dir='save_phi/', model_type='phi_1_5', pretrain_dir='iyubondyrev/jb_2024_kotlin_phi-1_5', config_dir=None, tokenizer_dir=None, load_name='pretrained', lit_file='../../../datasets/method_generation_dataset/kotlin/literals.json', mlm=False, mlm_probability=0.15, cache_dir='', block_size=512, do_train=True, do_eval=False, do_infer=False, evaluate_during_training=True, do_lower_case=False, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=12, gradient_accumulation_steps=1, learning_rate=3e-05, weight_decay=0.01, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=6.0, max_steps=-1, warmup_steps=1000, logging_steps=100, save_steps=1000, save_total_limit=None, eval_all_checkpoints=False, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, node_index=0, gpu_per_node=1, server_ip='', server_port='', log_file='method_gen_kotlin_train_my_phi-1_5.log', tensorboard_dir=None, lang='kotlin', n_gpu=1, device=device(type='cuda'), start_step=0)
Loading features from cached file save_phi/train_blocksize_512_wordsize_1_rank_0
***** Running training *****
  Num examples = 10597
  Num epoch = 5
  Instantaneous batch size per GPU = 8
  Total train batch size (w. parallel, distributed & accumulation) = 8
  Gradient Accumulation steps = 1
  Total optimization steps = 7944
  steps: 100  lr: 3e-06  loss: 2.2043
  steps: 200  lr: 6e-06  loss: 2.3515
  steps: 300  lr: 9e-06  loss: 2.1611
  steps: 400  lr: 1.2e-05  loss: 1.9492
  steps: 500  lr: 1.5e-05  loss: 1.8921
  steps: 600  lr: 1.8e-05  loss: 1.7352
  steps: 700  lr: 2.1e-05  loss: 1.6203
  steps: 800  lr: 2.4e-05  loss: 1.5338
  steps: 900  lr: 2.7e-05  loss: 1.4397
  steps: 1000  lr: 3e-05  loss: 1.3981
Data size: 669
Rank 0, load 0
Rank 0, load 10
Rank 0, load 20
Rank 0, load 30
Rank 0, load 40
Rank 0, load 50
Rank 0, load 60
Rank 0, load 70
Rank 0, load 80
Rank 0, load 90
Rank 0, load 100
Eval steps: 0
  perplexity = 1.753
Saving model checkpoint to save_phi/checkpoint-1000-1.753
Saving optimizer states to save_phi/checkpoint-last
  steps: 1100  lr: 3e-05  loss: 1.3795
  steps: 1200  lr: 2.9e-05  loss: 1.3598
  steps: 1300  lr: 2.9e-05  loss: 1.3675
  steps: 1400  lr: 2.8e-05  loss: 1.3343
  steps: 1500  lr: 2.8e-05  loss: 1.3217
  steps: 1600  lr: 2.7e-05  loss: 1.2894
  steps: 1700  lr: 2.7e-05  loss: 1.3158
  steps: 1800  lr: 2.7e-05  loss: 1.3184
  steps: 1900  lr: 2.6e-05  loss: 1.2905
  steps: 2000  lr: 2.6e-05  loss: 1.3167
Eval steps: 0
  perplexity = 1.6926
Saving model checkpoint to save_phi/checkpoint-2000-1.6926
Saving optimizer states to save_phi/checkpoint-last
  steps: 2100  lr: 2.5e-05  loss: 1.3168
  steps: 2200  lr: 2.5e-05  loss: 1.2735
