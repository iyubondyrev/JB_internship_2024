Model has a total of 124645632 trainable parameters
Training/evaluation parameters Namespace(data_dir='../../../datasets/kotlin', langs='kotlin', output_dir='save/', model_type='gpt2', pretrain_dir='microsoft/CodeGPT-small-py-adaptedGPT2', config_dir=None, tokenizer_dir=None, lit_file='../../../datasets/kotlin/literals.json', load_name='pretrained', mlm=False, mlm_probability=0.15, cache_dir='', block_size=1024, do_train=True, do_eval=False, evaluate_during_training=True, do_lower_case=False, per_gpu_train_batch_size=2, per_gpu_eval_batch_size=4, gradient_accumulation_steps=4, learning_rate=8e-05, weight_decay=0.01, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=5.0, max_steps=-1, warmup_steps=0, logging_steps=100, save_steps=1000, save_total_limit=None, eval_all_checkpoints=False, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=42, not_pretrain=True, fp16=False, fp16_opt_level='O1', local_rank=-1, node_index=-1, gpu_per_node=1, server_ip='', server_port='', log_file='completion_kotlin_eval.log', tensorboard_dir=None, wandb_run_name='EXP_1', wandb_project_name='JB_internship_2024', n_gpu=1, device=device(type='cuda'), start_epoch=0, start_step=0)
Creating features from dataset file at ../../../datasets/kotlin/train.txt
Data size: 39754
Rank 0, load 0
Rank 0, load 10
Rank 0, load 20
Rank 0, load 30
Rank 0, load 40
Rank 0, load 50
Rank 0, load 60
Rank 0, load 70
Rank 0, load 80
Rank 0, load 90
Rank 0, load 100
tokens: 22796815
Rank 0 Training 22796815 token, 22262 samples
Saving features into cached file save/train_blocksize_1024_wordsize_1_rank_0
***** Running training *****
  Num examples = 22262
  Num epoch = 4
  Instantaneous batch size per GPU = 2
  Total train batch size (w. parallel, distributed & accumulation) = 8
  Gradient Accumulation steps = 4
  Total optimization steps = 13910
Data size: 2487
load 0
load 10
load 20
load 30
load 40
load 50
load 60
load 70
load 80
load 90
load 100
tokens: 1496828
0 are done!
2233, 0.451858486341245
100 are done!
261649, 0.36323853712416254
200 are done!
527468, 0.36723175623924104
300 are done!
804476, 0.41510747368473394
Eval on 2487, saved at save/predictions.txt
Total 867460 tokens, accuracy: 45.42
  steps: 100  ppl: 5.5377  lr: 7.942487419122934e-05
  steps: 200  ppl: 3.7474  lr: 7.884974838245867e-05
Model has a total of 124645632 trainable parameters
Training/evaluation parameters Namespace(data_dir='../../../datasets/kotlin', langs='kotlin', output_dir='save/', model_type='gpt2', pretrain_dir='microsoft/CodeGPT-small-py-adaptedGPT2', config_dir=None, tokenizer_dir=None, lit_file='../../../datasets/kotlin/literals.json', load_name='pretrained', mlm=False, mlm_probability=0.15, cache_dir='', block_size=1024, do_train=True, do_eval=False, evaluate_during_training=True, do_lower_case=False, per_gpu_train_batch_size=16, per_gpu_eval_batch_size=16, gradient_accumulation_steps=2, learning_rate=8e-05, weight_decay=0.01, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=5.0, max_steps=-1, warmup_steps=0, logging_steps=100, save_steps=1000, save_total_limit=None, eval_all_checkpoints=False, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=42, not_pretrain=True, fp16=False, fp16_opt_level='O1', local_rank=-1, node_index=-1, gpu_per_node=1, server_ip='', server_port='', log_file='completion_kotlin_eval.log', tensorboard_dir=None, wandb_run_name='EXP_1', wandb_project_name='JB_internship_2024', n_gpu=1, device=device(type='cuda'), start_epoch=0, start_step=0)
Loading features from cached file save/train_blocksize_1024_wordsize_1_rank_0
***** Running training *****
  Num examples = 22262
  Num epoch = 4
  Instantaneous batch size per GPU = 16
  Total train batch size (w. parallel, distributed & accumulation) = 32
  Gradient Accumulation steps = 2
  Total optimization steps = 3475
0 are done!
9830, 0.35615462868769077
Eval on 2487, saved at save/predictions.txt
Total 867460 tokens, accuracy: 45.42
Model has a total of 124645632 trainable parameters
Training/evaluation parameters Namespace(data_dir='../../../datasets/kotlin', langs='kotlin', output_dir='save/', model_type='gpt2', pretrain_dir='microsoft/CodeGPT-small-py-adaptedGPT2', config_dir=None, tokenizer_dir=None, lit_file='../../../datasets/kotlin/literals.json', load_name='pretrained', mlm=False, mlm_probability=0.15, cache_dir='', block_size=1024, do_train=True, do_eval=False, evaluate_during_training=True, do_lower_case=False, per_gpu_train_batch_size=12, per_gpu_eval_batch_size=12, gradient_accumulation_steps=3, learning_rate=8e-05, weight_decay=0.01, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=5.0, max_steps=-1, warmup_steps=0, logging_steps=100, save_steps=1000, save_total_limit=None, eval_all_checkpoints=False, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=42, not_pretrain=True, fp16=False, fp16_opt_level='O1', local_rank=-1, node_index=-1, gpu_per_node=1, server_ip='', server_port='', log_file='completion_kotlin_eval.log', tensorboard_dir=None, wandb_run_name='EXP_1', wandb_project_name='JB_internship_2024', n_gpu=1, device=device(type='cuda'), start_epoch=0, start_step=0)
Loading features from cached file save/train_blocksize_1024_wordsize_1_rank_0
***** Running training *****
  Num examples = 22262
  Num epoch = 4
  Instantaneous batch size per GPU = 12
  Total train batch size (w. parallel, distributed & accumulation) = 36
  Gradient Accumulation steps = 3
  Total optimization steps = 3090
0 are done!
7357, 0.35829821938290063
100 are done!
809550, 0.41479216848866657
Eval on 2487, saved at save/predictions.txt
Total 867460 tokens, accuracy: 45.42
Model has a total of 124645632 trainable parameters
Training/evaluation parameters Namespace(data_dir='../../../datasets/kotlin', langs='kotlin', output_dir='save/', model_type='gpt2', pretrain_dir='microsoft/CodeGPT-small-py-adaptedGPT2', config_dir=None, tokenizer_dir=None, lit_file='../../../datasets/kotlin/literals.json', load_name='pretrained', mlm=False, mlm_probability=0.15, cache_dir='', block_size=1024, do_train=True, do_eval=False, evaluate_during_training=True, do_lower_case=False, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=8, gradient_accumulation_steps=4, learning_rate=8e-05, weight_decay=0.01, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=5.0, max_steps=-1, warmup_steps=0, logging_steps=100, save_steps=1000, save_total_limit=None, eval_all_checkpoints=False, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=42, not_pretrain=True, fp16=False, fp16_opt_level='O1', local_rank=-1, node_index=-1, gpu_per_node=1, server_ip='', server_port='', log_file='completion_kotlin_eval.log', tensorboard_dir=None, wandb_run_name='EXP_1', wandb_project_name='JB_internship_2024', n_gpu=1, device=device(type='cuda'), start_epoch=0, start_step=0)
Loading features from cached file save/train_blocksize_1024_wordsize_1_rank_0
***** Running training *****
  Num examples = 22262
  Num epoch = 4
  Instantaneous batch size per GPU = 8
  Total train batch size (w. parallel, distributed & accumulation) = 32
  Gradient Accumulation steps = 4
  Total optimization steps = 3475
0 are done!
5026, 0.38658973338639074
100 are done!
529928, 0.367398967406893
Eval on 2487, saved at save/predictions.txt
Total 867460 tokens, accuracy: 45.42
Model has a total of 124645632 trainable parameters
Training/evaluation parameters Namespace(data_dir='../../../datasets/kotlin', langs='kotlin', output_dir='save/', model_type='gpt2', pretrain_dir='microsoft/CodeGPT-small-py-adaptedGPT2', config_dir=None, tokenizer_dir=None, lit_file='../../../datasets/kotlin/literals.json', load_name='pretrained', mlm=False, mlm_probability=0.15, cache_dir='', block_size=1024, do_train=True, do_eval=False, evaluate_during_training=True, do_lower_case=False, per_gpu_train_batch_size=4, per_gpu_eval_batch_size=8, gradient_accumulation_steps=4, learning_rate=8e-05, weight_decay=0.01, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=5.0, max_steps=-1, warmup_steps=0, logging_steps=100, save_steps=1000, save_total_limit=None, eval_all_checkpoints=False, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=42, not_pretrain=True, fp16=False, fp16_opt_level='O1', local_rank=-1, node_index=-1, gpu_per_node=1, server_ip='', server_port='', log_file='completion_kotlin_eval.log', tensorboard_dir=None, wandb_run_name='EXP_1', wandb_project_name='JB_internship_2024', n_gpu=1, device=device(type='cuda'), start_epoch=0, start_step=0)
Loading features from cached file save/train_blocksize_1024_wordsize_1_rank_0
***** Running training *****
  Num examples = 22262
  Num epoch = 4
  Instantaneous batch size per GPU = 4
  Total train batch size (w. parallel, distributed & accumulation) = 16
  Gradient Accumulation steps = 4
  Total optimization steps = 6955
0 are done!
5026, 0.38658973338639074
100 are done!
529928, 0.367398967406893
Eval on 2487, saved at save/predictions.txt
Total 867460 tokens, accuracy: 45.42
  steps: 100  ppl: 5.2398  lr: 7.884974838245867e-05
  steps: 200  ppl: 3.5588  lr: 7.769949676491733e-05
  steps: 300  ppl: 3.3193  lr: 7.654924514737599e-05
  steps: 400  ppl: 3.1516  lr: 7.539899352983466e-05
  steps: 500  ppl: 2.9803  lr: 7.424874191229332e-05
  steps: 600  ppl: 2.902  lr: 7.309849029475199e-05
  steps: 700  ppl: 2.8695  lr: 7.194823867721066e-05
  steps: 800  ppl: 2.8076  lr: 7.079798705966932e-05
  steps: 900  ppl: 2.7484  lr: 6.964773544212798e-05
  steps: 1000  ppl: 2.6979  lr: 6.849748382458664e-05
0 are done!
5026, 0.6605650616792678
100 are done!
529928, 0.6277683005993268
Eval on 2487, saved at save/predictions.txt
Total 867460 tokens, accuracy: 73.46
Data size: 2487
Rank 0, load 0
Rank 0, load 10
Rank 0, load 20
Rank 0, load 30
Rank 0, load 40
Rank 0, load 50
Rank 0, load 60
Rank 0, load 70
Rank 0, load 80
Rank 0, load 90
Rank 0, load 100
tokens: 1496828
  perplexity = 2.4715
Saving model checkpoint to save/checkpoint-1000-2.4715
Saving optimizer and scheduler states to save/checkpoint-last
  steps: 1100  ppl: 2.6442  lr: 6.734723220704529e-05
  steps: 1200  ppl: 2.588  lr: 6.619698058950395e-05
  steps: 1300  ppl: 2.5969  lr: 6.504672897196261e-05
  steps: 1400  ppl: 2.6101  lr: 6.389647735442128e-05
  steps: 1500  ppl: 2.4825  lr: 6.274622573687994e-05
  steps: 1600  ppl: 2.516  lr: 6.15959741193386e-05
  steps: 1700  ppl: 2.4629  lr: 6.0445722501797276e-05
  steps: 1800  ppl: 2.5012  lr: 5.929547088425594e-05
  steps: 1900  ppl: 2.4406  lr: 5.81452192667146e-05
  steps: 2000  ppl: 2.4551  lr: 5.699496764917326e-05
0 are done!
5026, 0.6752884998010347
100 are done!
529928, 0.6402190486254736
Eval on 2487, saved at save/predictions.txt
Total 867460 tokens, accuracy: 74.72
  perplexity = 2.2974
Saving model checkpoint to save/checkpoint-2000-2.2974
Saving optimizer and scheduler states to save/checkpoint-last
  steps: 2100  ppl: 2.4261  lr: 5.5844716031631925e-05
  steps: 2200  ppl: 2.4139  lr: 5.469446441409059e-05
  steps: 2300  ppl: 2.3881  lr: 5.354421279654925e-05
  steps: 2400  ppl: 2.4145  lr: 5.239396117900791e-05
  steps: 2500  ppl: 2.3955  lr: 5.1243709561466573e-05
  steps: 2600  ppl: 2.3646  lr: 5.0093457943925236e-05
  steps: 2700  ppl: 2.3679  lr: 4.89432063263839e-05
  steps: 2800  ppl: 2.3491  lr: 4.779295470884257e-05
  steps: 2900  ppl: 2.3079  lr: 4.664270309130123e-05
  steps: 3000  ppl: 2.291  lr: 4.549245147375989e-05
0 are done!
5026, 0.6788698766414644
100 are done!
529928, 0.6466897389834091
Eval on 2487, saved at save/predictions.txt
Total 867460 tokens, accuracy: 75.46
  perplexity = 2.2128
Saving model checkpoint to save/checkpoint-3000-2.2128
Saving optimizer and scheduler states to save/checkpoint-last
  steps: 3100  ppl: 2.3005  lr: 4.434219985621855e-05
  steps: 3200  ppl: 2.2904  lr: 4.3191948238677215e-05
  steps: 3300  ppl: 2.2974  lr: 4.204169662113588e-05
  steps: 3400  ppl: 2.2294  lr: 4.089144500359454e-05
  steps: 3500  ppl: 2.2985  lr: 3.97411933860532e-05
  steps: 3600  ppl: 2.2676  lr: 3.8590941768511864e-05
  steps: 3700  ppl: 2.2784  lr: 3.7440690150970526e-05
  steps: 3800  ppl: 2.285  lr: 3.629043853342919e-05
  steps: 3900  ppl: 2.2536  lr: 3.514018691588785e-05
  steps: 4000  ppl: 2.2441  lr: 3.398993529834651e-05
0 are done!
5026, 0.6842419419021091
100 are done!
529928, 0.6521716157666702
Eval on 2487, saved at save/predictions.txt
Total 867460 tokens, accuracy: 76.05
  perplexity = 2.1603
Saving model checkpoint to save/checkpoint-4000-2.1603
Saving optimizer and scheduler states to save/checkpoint-last
  steps: 4100  ppl: 2.2296  lr: 3.2839683680805174e-05
  steps: 4200  ppl: 2.2621  lr: 3.168943206326384e-05
  steps: 4300  ppl: 2.2132  lr: 3.0539180445722505e-05
  steps: 4400  ppl: 2.231  lr: 2.9388928828181168e-05
  steps: 4500  ppl: 2.1745  lr: 2.823867721063983e-05
  steps: 4600  ppl: 2.2116  lr: 2.7088425593098492e-05
  steps: 4700  ppl: 2.1755  lr: 2.5938173975557157e-05
  steps: 4800  ppl: 2.1999  lr: 2.478792235801582e-05
  steps: 4900  ppl: 2.1704  lr: 2.363767074047448e-05
  steps: 5000  ppl: 2.2158  lr: 2.2487419122933144e-05
0 are done!
5026, 0.6834460803820135
100 are done!
529928, 0.6539567639377425
Eval on 2487, saved at save/predictions.txt
Total 867460 tokens, accuracy: 76.22
  perplexity = 2.132
Saving model checkpoint to save/checkpoint-5000-2.132
Saving optimizer and scheduler states to save/checkpoint-last
  steps: 5100  ppl: 2.1924  lr: 2.1337167505391806e-05
  steps: 5200  ppl: 2.1832  lr: 2.0186915887850468e-05
  steps: 5300  ppl: 2.1731  lr: 1.903666427030913e-05
  steps: 5400  ppl: 2.1953  lr: 1.7886412652767796e-05
  steps: 5500  ppl: 2.1667  lr: 1.6736161035226458e-05
  steps: 5600  ppl: 2.1607  lr: 1.558590941768512e-05
  steps: 5700  ppl: 2.1499  lr: 1.4435657800143784e-05
  steps: 5800  ppl: 2.1481  lr: 1.3285406182602444e-05
  steps: 5900  ppl: 2.1506  lr: 1.213515456506111e-05
  steps: 6000  ppl: 2.1692  lr: 1.0984902947519772e-05
0 are done!
5026, 0.6894150417827298
100 are done!
529928, 0.6558891019157319
Eval on 2487, saved at save/predictions.txt
Total 867460 tokens, accuracy: 76.44
  perplexity = 2.1121
Saving model checkpoint to save/checkpoint-6000-2.1121
Saving optimizer and scheduler states to save/checkpoint-last
  steps: 6100  ppl: 2.1689  lr: 9.834651329978434e-06
  steps: 6200  ppl: 2.1685  lr: 8.684399712437096e-06
  steps: 6300  ppl: 2.1398  lr: 7.534148094895759e-06
  steps: 6400  ppl: 2.1517  lr: 6.383896477354422e-06
  steps: 6500  ppl: 2.1533  lr: 5.233644859813084e-06
  steps: 6600  ppl: 2.1352  lr: 4.083393242271747e-06
  steps: 6700  ppl: 2.14  lr: 2.93314162473041e-06
  steps: 6800  ppl: 2.1348  lr: 1.7828900071890728e-06
  steps: 6900  ppl: 2.1413  lr: 6.326383896477354e-07
 global_step = 6955, average loss = 0.8713822233481942
Model has a total of 1415660931 trainable parameters
Training/evaluation parameters Namespace(data_dir='../../../datasets/kotlin', langs='kotlin', output_dir='save/', model_type='phi_1_5', pretrain_dir='microsoft/phi-1_5', config_dir=None, tokenizer_dir=None, lit_file='../../../datasets/kotlin/literals.json', load_name='pretrained', mlm=False, mlm_probability=0.15, cache_dir='', block_size=512, do_train=True, do_eval=False, evaluate_during_training=True, do_lower_case=False, per_gpu_train_batch_size=4, per_gpu_eval_batch_size=8, gradient_accumulation_steps=4, learning_rate=8e-05, weight_decay=0.01, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=5.0, max_steps=-1, warmup_steps=0, logging_steps=100, save_steps=1000, save_total_limit=None, eval_all_checkpoints=False, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=42, not_pretrain=True, fp16=False, fp16_opt_level='O1', local_rank=-1, node_index=-1, gpu_per_node=1, server_ip='', server_port='', log_file='completion_kotlin_eval_vanilla_gpt_py.log', tensorboard_dir=None, wandb_run_name='PHI_1_5_FINETUNE', wandb_project_name='JB_internship_2024', n_gpu=1, device=device(type='cuda'), start_epoch=0, start_step=0)
Creating features from dataset file at ../../../datasets/kotlin/train.txt
Data size: 39754
Rank 0, load 0
Rank 0, load 10
Rank 0, load 20
Rank 0, load 30
Rank 0, load 40
Rank 0, load 50
Rank 0, load 60
Rank 0, load 70
Rank 0, load 80
Rank 0, load 90
Rank 0, load 100
tokens: 22796815
Rank 0 Training 22796815 token, 44525 samples
Saving features into cached file save/train_blocksize_512_wordsize_1_rank_0
***** Running training *****
  Num examples = 44525
  Num epoch = 4
  Instantaneous batch size per GPU = 4
  Total train batch size (w. parallel, distributed & accumulation) = 16
  Gradient Accumulation steps = 4
  Total optimization steps = 13910
Data size: 2487
load 0
load 10
load 20
load 30
load 40
load 50
load 60
load 70
load 80
load 90
load 100
tokens: 1496828
0 are done!
2227, 0.5779074988774136
100 are done!
261277, 0.4837318248448964
200 are done!
526617, 0.4887100112605556
300 are done!
803069, 0.5260245383646984
Eval on 2487, saved at save/predictions.txt
Total 867460 tokens, accuracy: 58.05
Model has a total of 1415660931 trainable parameters
Training/evaluation parameters Namespace(data_dir='../../../datasets/kotlin', langs='kotlin', output_dir='save/', model_type='phi_1_5', pretrain_dir='microsoft/phi-1_5', config_dir=None, tokenizer_dir=None, lit_file='../../../datasets/kotlin/literals.json', load_name='pretrained', mlm=False, mlm_probability=0.15, cache_dir='', block_size=512, do_train=True, do_eval=False, evaluate_during_training=True, do_lower_case=False, per_gpu_train_batch_size=4, per_gpu_eval_batch_size=4, gradient_accumulation_steps=4, learning_rate=8e-05, weight_decay=0.01, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=5.0, max_steps=-1, warmup_steps=0, logging_steps=100, save_steps=1000, save_total_limit=None, eval_all_checkpoints=False, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=42, not_pretrain=True, fp16=False, fp16_opt_level='O1', local_rank=-1, node_index=-1, gpu_per_node=1, server_ip='', server_port='', log_file='completion_kotlin_eval_vanilla_gpt_py.log', tensorboard_dir=None, wandb_run_name='PHI_1_5_FINETUNE', wandb_project_name='JB_internship_2024', n_gpu=1, device=device(type='cuda'), start_epoch=0, start_step=0)
Loading features from cached file save/train_blocksize_512_wordsize_1_rank_0
***** Running training *****
  Num examples = 44525
  Num epoch = 4
  Instantaneous batch size per GPU = 4
  Total train batch size (w. parallel, distributed & accumulation) = 16
  Gradient Accumulation steps = 4
  Total optimization steps = 13910
0 are done!
1159, 0.5798101811906816
100 are done!
129844, 0.4881704198884816
200 are done!
260023, 0.48397257165712265
300 are done!
393050, 0.48858923801043125
400 are done!
525219, 0.48883608551861224
500 are done!
666311, 0.5357648305370916
600 are done!
801699, 0.5261014420624199
700 are done!
930580, 0.5186109738012852
Eval on 2487, saved at save/predictions.txt
Total 867460 tokens, accuracy: 58.05
Model has a total of 1415660931 trainable parameters
Training/evaluation parameters Namespace(data_dir='../../../datasets/kotlin', langs='kotlin', output_dir='save/', model_type='phi_1_5', pretrain_dir='microsoft/phi-1_5', config_dir=None, tokenizer_dir=None, lit_file='../../../datasets/kotlin/literals.json', load_name='pretrained', mlm=False, mlm_probability=0.15, cache_dir='', block_size=512, do_train=True, do_eval=False, evaluate_during_training=True, do_lower_case=False, per_gpu_train_batch_size=2, per_gpu_eval_batch_size=4, gradient_accumulation_steps=8, learning_rate=8e-05, weight_decay=0.01, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=5.0, max_steps=-1, warmup_steps=0, logging_steps=100, save_steps=1000, save_total_limit=None, eval_all_checkpoints=False, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=42, not_pretrain=True, fp16=False, fp16_opt_level='O1', local_rank=-1, node_index=-1, gpu_per_node=1, server_ip='', server_port='', log_file='completion_kotlin_eval_vanilla_gpt_py.log', tensorboard_dir=None, wandb_run_name='PHI_1_5_FINETUNE', wandb_project_name='JB_internship_2024', n_gpu=1, device=device(type='cuda'), start_epoch=0, start_step=0)
Loading features from cached file save/train_blocksize_512_wordsize_1_rank_0
***** Running training *****
  Num examples = 44525
  Num epoch = 4
  Instantaneous batch size per GPU = 2
  Total train batch size (w. parallel, distributed & accumulation) = 16
  Gradient Accumulation steps = 8
  Total optimization steps = 13910
0 are done!
1159, 0.5798101811906816
100 are done!
129844, 0.4881704198884816
200 are done!
260023, 0.48397257165712265
300 are done!
393050, 0.48858923801043125
400 are done!
525219, 0.48883608551861224
500 are done!
666311, 0.5357648305370916
600 are done!
801699, 0.5261014420624199
700 are done!
930580, 0.5186109738012852
Eval on 2487, saved at save/predictions.txt
Total 867460 tokens, accuracy: 58.05
Model has a total of 1415660931 trainable parameters
Training/evaluation parameters Namespace(data_dir='../../../datasets/kotlin', langs='kotlin', output_dir='save/', model_type='phi_1_5', pretrain_dir='microsoft/phi-1_5', config_dir=None, tokenizer_dir=None, lit_file='../../../datasets/kotlin/literals.json', load_name='pretrained', mlm=False, mlm_probability=0.15, cache_dir='', block_size=512, do_train=True, do_eval=False, evaluate_during_training=True, do_lower_case=False, per_gpu_train_batch_size=2, per_gpu_eval_batch_size=2, gradient_accumulation_steps=8, learning_rate=8e-05, weight_decay=0.01, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=5.0, max_steps=-1, warmup_steps=0, logging_steps=100, save_steps=1000, save_total_limit=None, eval_all_checkpoints=False, no_cuda=False, overwrite_output_dir=True, overwrite_cache=False, seed=42, not_pretrain=True, fp16=False, fp16_opt_level='O1', local_rank=-1, node_index=-1, gpu_per_node=1, server_ip='', server_port='', log_file='completion_kotlin_eval_vanilla_gpt_py.log', tensorboard_dir=None, wandb_run_name='PHI_1_5_FINETUNE', wandb_project_name='JB_internship_2024', n_gpu=1, device=device(type='cuda'), start_epoch=0, start_step=0)
Loading features from cached file save/train_blocksize_512_wordsize_1_rank_0
***** Running training *****
  Num examples = 44525
  Num epoch = 4
  Instantaneous batch size per GPU = 2
  Total train batch size (w. parallel, distributed & accumulation) = 16
  Gradient Accumulation steps = 8
  Total optimization steps = 13910
0 are done!
643, 0.6189735614307932
100 are done!
64441, 0.48495523036576094
200 are done!
129199, 0.488548672977345
300 are done!
193258, 0.4834314750230262
400 are done!
259267, 0.4842536844257079
500 are done!
326182, 0.4901803287735068
600 are done!
392442, 0.48866329291971805
700 are done!
458569, 0.48660070785421605
800 are done!
524483, 0.48892719115776867
900 are done!
593484, 0.49830492481684424
1000 are done!
665579, 0.535729041931912
1100 are done!
732143, 0.5324765790289602
1200 are done!
800983, 0.5260361331014516
1300 are done!
865886, 0.5230619273206866
1400 are done!
930023, 0.5185559927012557
Eval on 2487, saved at save/predictions.txt
Total 867460 tokens, accuracy: 58.05
