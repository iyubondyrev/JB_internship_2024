{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import logging\n",
    "import gc\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "logger = logging.getLogger(\"cur_loger\")\n",
    "\n",
    "class EvalDataset(Dataset):\n",
    "    def __init__(self, tokenizer, output_dir, data_dir,  logger, file_type='test', block_size=512):\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        cached_file = os.path.join(output_dir, file_type+\"_blocksize_%d\"%(block_size))\n",
    "        if os.path.exists(cached_file):\n",
    "            with open(cached_file, 'rb') as handle:\n",
    "                self.inputs = pickle.load(handle)\n",
    "\n",
    "        else:\n",
    "            self.inputs = []\n",
    "\n",
    "            datafile = os.path.join(data_dir, f\"{file_type}.txt\")\n",
    "            with open(datafile) as f:\n",
    "                data = f.readlines()\n",
    "\n",
    "            length = len(data)\n",
    "            logger.info(\"Data size: %d\"%(length))\n",
    "            input_ids = []\n",
    "            for idx,x in enumerate(data):\n",
    "                x = x.strip()\n",
    "                if x.startswith(\"<s>\") and x.endswith(\"</s>\"):\n",
    "                    pass\n",
    "                else:\n",
    "                    x = \"<s> \" + x + \" </s>\"\n",
    "                try:\n",
    "                    input_ids.extend(tokenizer.encode(x))\n",
    "                except Exception:\n",
    "                    pass\n",
    "                if idx % (length//10) == 0:\n",
    "                    percent = idx / (length//10) * 10\n",
    "                    logger.warning(\"load %d\"%(percent))\n",
    "            del data\n",
    "            gc.collect()\n",
    "\n",
    "            logger.info(f\"tokens: {len(input_ids)}\")\n",
    "            self.split(input_ids, tokenizer, logger, block_size=block_size)\n",
    "            del input_ids\n",
    "            gc.collect()\n",
    "\n",
    "            with open(cached_file, 'wb') as handle:\n",
    "                pickle.dump(self.inputs, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    def split(self, input_ids, tokenizer, logger, block_size=1024):\n",
    "        sample = []\n",
    "        i = 0\n",
    "        while i < len(input_ids):\n",
    "            sample = input_ids[i: i+block_size]\n",
    "            if len(sample) == block_size:\n",
    "                for j in range(block_size):\n",
    "                    if tokenizer.convert_ids_to_tokens(sample[block_size-1-j])[0] == '\\u0120' or tokenizer.convert_ids_to_tokens(sample[block_size-1-j]).startswith(\"<NUM_LIT\"):\n",
    "                        break\n",
    "                    if sample[block_size-1-j] in [tokenizer.bos_token_id, tokenizer.eos_token_id, tokenizer.sep_token_id]:\n",
    "                        if sample[block_size-1-j] != tokenizer.bos_token_id:\n",
    "                            j -= 1\n",
    "                        break\n",
    "                if j == block_size-1:\n",
    "                    print(tokenizer.decode(sample))\n",
    "                    exit()\n",
    "                sample = sample[: block_size-1-j]\n",
    "            # print(len(sample))\n",
    "            i += len(sample)\n",
    "            pad_len = block_size-len(sample)\n",
    "            sample += [tokenizer.pad_token_id]*pad_len\n",
    "            self.inputs.append(sample)\n",
    "\n",
    "            if len(self.inputs) % 10000 == 0:\n",
    "                logger.info(f\"{len(self.inputs)} samples\")\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return torch.tensor(self.inputs[item])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset, SequentialSampler, RandomSampler,TensorDataset\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "distributed = False\n",
    "n_gpu = 1\n",
    "local_rank = -1\n",
    "gpu_per_node = -1\n",
    "per_gpu_eval_batch_size = 16\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "logging_steps = 10\n",
    "logger = logging.getLogger(\"cur_logger\")\n",
    "\n",
    "\n",
    "def post_process(preds, gts, true_gts, saved_file):\n",
    "    wf = open(saved_file, \"w\")\n",
    "\n",
    "    cnt = 0\n",
    "    new_gt = []\n",
    "    new_pred = []\n",
    "    for i, (pred,gt) in enumerate(zip(preds,gts)):\n",
    "        if gt in [\"\", \"<pad>\"]:\n",
    "            continue\n",
    "        new_gt.append(gt)\n",
    "        new_pred.append(pred.replace(\" \", \"\"))\n",
    "        if gt == \"</s>\":\n",
    "            gt_str = \" \".join(new_gt)\n",
    "            pred_str = \" \".join(new_pred)\n",
    "            assert gt_str == true_gts[cnt].strip(), f\"{cnt} sample gt_str != true_gt\"\n",
    "            wf.write(pred_str+\"\\n\")\n",
    "            cnt += 1\n",
    "            new_gt = []\n",
    "            new_pred = []\n",
    "    \n",
    "    return cnt\n",
    "\n",
    "def eval_acc(model, tokenizer, output_dir, data_dir, file_type='test'):\n",
    "    eval_dataset = EvalDataset(\n",
    "                            tokenizer=tokenizer,\n",
    "                            output_dir=output_dir,\n",
    "                            data_dir=data_dir,\n",
    "                            logger=logger,\n",
    "                          )\n",
    "\n",
    "\n",
    "    eval_batch_size = per_gpu_eval_batch_size * max(1, n_gpu)\n",
    "    eval_sampler = SequentialSampler(eval_dataset)\n",
    "    eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=eval_batch_size)\n",
    "    model.to(device)\n",
    "\n",
    "    if n_gpu > 1:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "\n",
    "    def DecodeIds(idxs):\n",
    "        codes = \"\"\n",
    "        for idx in idxs:\n",
    "            to_add = tokenizer.convert_ids_to_tokens(idx)\n",
    "            if tokenizer.convert_ids_to_tokens(idx)[0] == '\\u0120':\n",
    "                if not codes.endswith(\" \"):\n",
    "                    codes += \" \" + to_add[1:]\n",
    "                else:\n",
    "                    codes += to_add[1:]\n",
    "            elif (\n",
    "                idx in [tokenizer.bos_token_id, tokenizer.eos_token_id, tokenizer.sep_token_id, tokenizer.pad_token_id] or\n",
    "                tokenizer.convert_ids_to_tokens(idx).startswith(\"<NUM_LIT\")\n",
    "            ):\n",
    "                codes += \" \" + to_add + \" \"\n",
    "            else:\n",
    "                codes += to_add\n",
    "        return codes.strip(\" \")\n",
    "    \n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0.0\n",
    "    total = 0\n",
    "\n",
    "    total_pred = []\n",
    "    total_gt = []\n",
    "\n",
    "    for step, batch in tqdm(enumerate(eval_dataloader), total=len(eval_dataloader)):\n",
    "        inputs = batch.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs)\n",
    "            pred_scores = outputs[0]\n",
    "            pred_ids = pred_scores.argmax(-1)\n",
    "\n",
    "        all_pred = []\n",
    "        all_gt = []\n",
    "        prev_pred = None\n",
    "        for pred, gt in zip(pred_ids, inputs):\n",
    "            pred = pred.cpu().tolist()\n",
    "            gt = gt.cpu().tolist()\n",
    "\n",
    "            for i, y in enumerate(gt):\n",
    "                if i == 0:\n",
    "                    if y in [tokenizer.bos_token_id, tokenizer.eos_token_id, tokenizer.sep_token_id, tokenizer.pad_token_id]:\n",
    "                        now_gt = [y]\n",
    "                        now_pred = [0] if prev_pred is None else [prev_pred]\n",
    "                        all_pred.append(DecodeIds(now_pred).strip().split()[0])\n",
    "                        all_gt.append(DecodeIds(now_gt).strip())\n",
    "                        now_gt = []\n",
    "                        now_pred = []\n",
    "                    else:\n",
    "                        now_gt = [y]\n",
    "                        now_pred = [0] if prev_pred is None else [prev_pred]\n",
    "                else:\n",
    "                    if tokenizer.convert_ids_to_tokens(y)[0] == '\\u0120':\n",
    "                        if len(now_gt) > 0:\n",
    "                            try:\n",
    "                                all_pred.append(DecodeIds(now_pred).strip().split()[0])\n",
    "                            except IndexError:\n",
    "                                all_pred.append(\"<SPACE>\")\n",
    "                            all_gt.append(DecodeIds(now_gt).strip())\n",
    "                            now_gt = []\n",
    "                            now_pred = []\n",
    "                    if y in [tokenizer.bos_token_id, tokenizer.eos_token_id, tokenizer.sep_token_id, tokenizer.pad_token_id] or tokenizer.convert_ids_to_tokens(y).startswith(\"<NUM_LIT\"):\n",
    "                        if len(now_gt) > 0:\n",
    "                            try:\n",
    "                                all_pred.append(DecodeIds(now_pred).strip().split()[0])\n",
    "                            except IndexError:\n",
    "                                all_pred.append(\"<SPACE>\")\n",
    "                            all_gt.append(DecodeIds(now_gt).strip())\n",
    "                        now_gt = [y]\n",
    "                        now_pred = [pred[i-1]]\n",
    "                        try:\n",
    "                            all_pred.append(DecodeIds(now_pred).strip().split()[0])\n",
    "                        except IndexError:\n",
    "                            all_pred.append(\"<SPACE>\")\n",
    "                        all_gt.append(DecodeIds(now_gt).strip())\n",
    "                        now_gt = []\n",
    "                        now_pred = []\n",
    "                        continue\n",
    "                    now_gt.append(y)\n",
    "                    now_pred.append(pred[i-1])\n",
    "        assert len(all_pred) == len(all_gt)\n",
    "\n",
    "        total_pred.extend(all_pred)\n",
    "        total_gt.extend(all_gt)\n",
    "\n",
    "\n",
    "        for x, y in zip(all_pred, all_gt):\n",
    "            if y not in [\"<s>\", \"</s>\", \"<EOL>\", \"<pad>\"]:\n",
    "                total += 1\n",
    "                if x == y:\n",
    "                    correct += 1\n",
    "        \n",
    "        if step % logging_steps == 0:\n",
    "            logger.info(f\"{step} are done!\")\n",
    "            logger.info(f\"{total}, {correct/total}\")\n",
    "\n",
    "    # pickle.dump(total_pred, open(os.path.join(args.output_dir, \"preds.pkl\"), \"wb\"))\n",
    "    # pickle.dump(total_gt, open(os.path.join(args.output_dir, \"gts.pkl\"), \"wb\"))\n",
    "\n",
    "    saved_file = os.path.join(output_dir, \"predictions.txt\")\n",
    "    total_samples = post_process(total_pred, total_gt, open(os.path.join(data_dir, f\"{file_type}.txt\")).readlines(), saved_file)\n",
    "    logger.info(f\"Eval on {total_samples}, saved at {saved_file}\")\n",
    "    \n",
    "    return total, correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50533, 2048)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, PhiForCausalLM\n",
    "import json\n",
    "\n",
    "torch.set_default_device(\"cuda\")\n",
    "\n",
    "# python tokenizer\n",
    "\n",
    "def get_special_tokens(path):\n",
    "    lits = json.load(open(path))\n",
    "    tokens = [\"<STR_LIT>\", \"<NUM_LIT>\", \"<CHAR_LIT>\"]\n",
    "    for lit in lits[\"str\"]:\n",
    "        tokens.append(f\"<STR_LIT:{lit}>\")\n",
    "    for lit in lits[\"num\"]:\n",
    "        tokens.append(f\"<NUM_LIT:{lit}>\")\n",
    "    for lit in lits[\"char\"]:\n",
    "        tokens.append(f\"<CHAR_LIT:{lit}>\")\n",
    "    return tokens\n",
    "\n",
    "special_tokens = get_special_tokens(\"CodeXGLUE_python/CodeCompletion-token/dataset/py150/literals.json\")\n",
    "\n",
    "python_tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-1_5\",\n",
    "                                            sep_token='<EOL>',\n",
    "                                            bos_token='<s>',\n",
    "                                            eos_token='</s>',\n",
    "                                            pad_token='<pad>',\n",
    "                                            unk_token='<|UNKNOWN|>',\n",
    "                                            additional_special_tokens=special_tokens\n",
    "                                            )\n",
    "\n",
    "# model\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-1_5\", torch_dtype=\"auto\")\n",
    "\n",
    "\n",
    "model.resize_token_embeddings(len(python_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8abae4050e5458296ff05dc4a8db994",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8209 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43meval_acc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpython_tokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutput_dir\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdatasets/python\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 93\u001b[0m, in \u001b[0;36meval_acc\u001b[0;34m(model, tokenizer, output_dir, data_dir, file_type)\u001b[0m\n\u001b[1;32m     91\u001b[0m prev_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pred, gt \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(pred_ids, inputs):\n\u001b[0;32m---> 93\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[43mpred\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m     94\u001b[0m     gt \u001b[38;5;241m=\u001b[39m gt\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, y \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(gt):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/_device.py:77\u001b[0m, in \u001b[0;36mDeviceContext.__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m _device_constructors() \u001b[38;5;129;01mand\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     76\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "eval_acc(\n",
    "    model=model,\n",
    "    tokenizer=python_tokenizer,\n",
    "    output_dir=\"output_dir\",\n",
    "    data_dir=\"datasets/python\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
